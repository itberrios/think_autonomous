{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MVS_Run.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "m-h9g7awqda4"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/itberrios/think_autonomous/blob/main/3d_reconstruction/MVS_Run.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Multi View Stereo Notebook\n",
        "Welcome to the Multi-View Stereo Notebook!\n",
        "Here, you're going to learn how to build Multi-View Reconstruction algorithm from multiple images; such as a 360° video of an object!\n",
        "<p>\n",
        "\n",
        "**Your input will be a set of images, and your output will be a 3D Point Cloud.**\n",
        "\n",
        "For example, you'll learn how to turn a series of images from this temple into a point cloud:\n",
        "![img](https://perso.telecom-paristech.fr/boubek/papers/AMR/images/amr_scaled-window-matching.png)\n",
        "\n",
        "Some of the elements of the \"Rock\" notebook, including the dataset, have been inspired from this fantastic repo: https://github.com/drewm1980/multi_view_stereo_benchmark\n",
        "\n",
        "**A warning** — The topic is difficult, and it's likely that if you scatter the web, you won't find easy and perfectly working Python implementations of Multi-View Stereo algorithms. So *be patient*.\n",
        "\n",
        "**However, if you understand how to do 2-View Reconstruction with Rectification and Disparity Estimation, then this should be straightforward.**\n",
        "\n",
        "Now, let's get started!"
      ],
      "metadata": {
        "id": "5dEVMdJi3WAi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How it works:\n",
        "1.   Create the parameters for: **calibration, rectification, disparity estimation**\n",
        "2.   Create a class that will **collect all parameters**.\n",
        "3.   Load the **Images**\n",
        "4.   Test **Rectification, Disparity, and Reprojection**\n",
        "5.   Putting it **all together**\n"
      ],
      "metadata": {
        "id": "pChelOSjcJV0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0 — Intro"
      ],
      "metadata": {
        "id": "WHRb70224jir"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Imports"
      ],
      "metadata": {
        "id": "Z1gdE1KIdFBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://stereo-vision.s3.eu-west-3.amazonaws.com/mvs.zip && unzip mvs.zip && rm mvs.zip"
      ],
      "metadata": {
        "id": "qOCDphK6-RMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv mvs/* . && rm -r mvs"
      ],
      "metadata": {
        "id": "cztT9erZUG2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhm2NpmF2xMS"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "import pathlib\n",
        "from pathlib import Path\n",
        "from time import time\n",
        "import os\n",
        "import inspect\n",
        "pwd = os.path.dirname(os.path.abspath(inspect.stack()[0][1]))\n",
        "from load_ply import save_ply\n",
        "from load_camera_info import load_intrinsics, load_extrinsics\n",
        "import cv2\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import cv2_imshow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scenario = \"rock\" # rock or temple"
      ],
      "metadata": {
        "id": "Xkz_yPsJs1g9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Visualization"
      ],
      "metadata": {
        "id": "Rx3viXpPUeV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images = sorted(glob.glob(\"data/\"+scenario+\"/undistorted/*.png\"))\n",
        "\n",
        "index = 10\n",
        "\n",
        "plt.imshow(cv2.imread(images[index]))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "upJFnZq_UfwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if scenario == \"rock\":\n",
        "    images_cv = [cv2.rotate(cv2.cvtColor(cv2.imread(img), cv2.COLOR_BGR2RGB), cv2.ROTATE_180) for img in images]\n",
        "    #images_cv = [cv2.cvtColor(cv2.imread(img), cv2.COLOR_BGR2RGB) for img in images]\n",
        "\n",
        "elif scenario==\"temple\":\n",
        "    #images_cv = [cv2.rotate(cv2.cvtColor(cv2.imread(img), cv2.COLOR_BGR2RGB), cv2.ROTATE_90_COUNTERCLOCKWISE) for img in images]\n",
        "    images_cv = [cv2.cvtColor(cv2.imread(img), cv2.COLOR_BGR2RGB) for img in images]\n",
        "\n",
        "plt.imshow(images_cv[index])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xIN_-Uoy5KsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f, ((ax0, ax1, ax2, ax3, ax4, ax5), (ax6, ax7, ax8, ax9, ax10, ax11)) = plt.subplots(2,6,figsize=(20,10))\n",
        "ax0.imshow(images_cv[0])\n",
        "ax1.imshow(images_cv[1])\n",
        "ax2.imshow(images_cv[2])\n",
        "ax3.imshow(images_cv[3])\n",
        "ax4.imshow(images_cv[4])\n",
        "ax5.imshow(images_cv[5])\n",
        "ax6.imshow(images_cv[6])\n",
        "ax7.imshow(images_cv[7])\n",
        "ax8.imshow(images_cv[8])\n",
        "ax9.imshow(images_cv[9])\n",
        "ax10.imshow(images_cv[10])\n",
        "ax11.imshow(images_cv[11])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Tkzs69Wp5wA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h, w, d = images_cv[index].shape\n",
        "\n",
        "h, w, d"
      ],
      "metadata": {
        "id": "ong3MmuTU_JP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1 — Parameters\n",
        "*   Topology\n",
        "*   Rectification\n",
        "*   Disparity Estimation\n",
        "*   Camera Calibration"
      ],
      "metadata": {
        "id": "mtytU2474mW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Topologies"
      ],
      "metadata": {
        "id": "SL3Sbv_HIGN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "topologies = collections.OrderedDict()\n",
        "topologies['360'] = tuple(zip((0,1,2,3,4,5,6,7,8,9,10,11),\n",
        "                          (1,2,3,4,5,6,7,8,9,10,11,0)))\n",
        "\n",
        "topologies['overlapping'] = tuple(zip((0,1,2,3,4,5,6,7,8,9,10),\n",
        "                          (1,2,3,4,5,6,7,8,9,10,11)))\n",
        "\n",
        "topologies['adjacent'] = tuple(zip((0,2,4,6,8,10),\n",
        "                     (1,3,5,7,9,11)))\n",
        "topologies['skipping_1'] = tuple(zip((0,3,6,9),\n",
        "                 (1,4,7,10)))\n",
        "topologies['skipping_2'] = tuple(zip((0,4,8),\n",
        "                 (1,5,9)))\n"
      ],
      "metadata": {
        "id": "MHi0NFGO4oG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for pair_index, (left_index,right_index) in enumerate(topologies[\"adjacent\"]):\n",
        "    print(left_index, right_index)"
      ],
      "metadata": {
        "id": "oXeWAYgVIBuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Camera Calibration & Rectification"
      ],
      "metadata": {
        "id": "GZNfSEvldS5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "StereoRectifyOptions = {'imageSize':(w,h),\n",
        "                        'flags':(0,cv2.CALIB_ZERO_DISPARITY)[0], # TODO explore other flags\n",
        "                        'newImageSize':(w,h),\n",
        "                        'alpha':0.5}\n",
        "\n",
        "RemapOptions = {'interpolation':cv2.INTER_LINEAR}\n",
        "\n",
        "CameraArrayOptions = {'channels':1,'num_cameras':12,'topology':'adjacent'}"
      ],
      "metadata": {
        "id": "Muf7vKmddjaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Disparity Estimation"
      ],
      "metadata": {
        "id": "J39HrJj7TPpM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "StereoMatcherOptions = {'MinDisparity': -64, # Influences MAX depth\n",
        "                        'NumDisparities': 256, # Influences MIN depth\n",
        "                        'BlockSize': 21,\n",
        "                        'SpeckleWindowSize': 0, # Must be strictly positive to turn on speckle post-filter.\n",
        "                        'SpeckleRange': 0, # Must be >= 0 to enable speckle post-filter\n",
        "                        'Disp12MaxDiff': 0}\n",
        "\n",
        "StereoBMOptions = {\n",
        "        'PreFilterType': (cv2.StereoBM_PREFILTER_NORMALIZED_RESPONSE, cv2.StereoBM_PREFILTER_XSOBEL)[0],\n",
        "                   'PreFilterSize': 5, # preFilterSize must be odd and be within 5..255\n",
        "                   'PreFilterCap': 63, # preFilterCap must be within 1..63. Used to truncate pixel values\n",
        "                   'TextureThreshold': 10,\n",
        "                   'UniquenessRatio': 10,\n",
        "                   }\n",
        "\n",
        "StereoSGBMOptions = {'PreFilterCap': 0,\n",
        "                     'UniquenessRatio': 0,\n",
        "                     'P1': 16*21*21, # \"Depth Change Cost in Ensenso terminology\"\n",
        "                     'P2': 16*21*21, # \"Depth Step Cost in Ensenso terminology\"\n",
        "                     'Mode': (cv2.StereoSGBM_MODE_SGBM, cv2.StereoSGBM_MODE_HH,\n",
        "                              cv2.StereoSGBM_MODE_SGBM_3WAY)[1]}\n",
        "\n",
        "\n",
        "FinalOptions = {'StereoRectify':StereoRectifyOptions,\n",
        "        'StereoMatcher':StereoMatcherOptions,\n",
        "        'StereoBM':StereoBMOptions,#change to \"StereoSGBM\" = StereoSGBMOptions if needed\n",
        "        'CameraArray':CameraArrayOptions,\n",
        "        'Remap':RemapOptions}"
      ],
      "metadata": {
        "id": "5JAPmxH4rm2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2 — Create a Class that will Collect all parameters\n",
        "*   Init: Get parameters, get remap options, get rectification options, get Q, convert to global coordinates, get disparity matcher\n",
        "*   Load images (part 3)\n",
        "*   Run the other functions (part 4, 5)"
      ],
      "metadata": {
        "id": "_m8XiID44qVI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Helper Function: Get the parameters"
      ],
      "metadata": {
        "id": "fjQEIFKDjcT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_camera_parameters(options):\n",
        "    flags=options['StereoRectify']['flags']\n",
        "    distortion_coefficients = (0.0,0.0,0.0,0.0,0.0)\n",
        "    left_distortion_coefficients = distortion_coefficients\n",
        "    right_distortion_coefficients = distortion_coefficients\n",
        "    imageSize = options['StereoRectify']['imageSize'] # w,h\n",
        "    newImageSize = options['StereoRectify']['newImageSize']\n",
        "    alpha = options['StereoRectify']['alpha']\n",
        "    return left_distortion_coefficients, right_distortion_coefficients, imageSize, newImageSize, alpha"
      ],
      "metadata": {
        "id": "YUcgJws4d_tX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Helper Function: Calibrate & Rectify"
      ],
      "metadata": {
        "id": "IfLfUKO-jff0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calibrate_and_rectify(options, left_K, right_K, left_R, right_R, left_T, right_T):\n",
        "    # Get the parameters\n",
        "    left_distortion_coefficients, right_distortion_coefficients, imageSize, newImageSize, alpha = get_camera_parameters(options)\n",
        "\n",
        "    # Stereo Rectify\n",
        "    R_intercamera = numpy.dot(right_R, left_R.T) # R * T\n",
        "    T_intercamera = right_T - numpy.dot(R_intercamera, left_T) # translation and rotation keeping the first one as baseline\n",
        "\n",
        "    left_R_rectified, right_R_rectified, P1_rect, P2_rect, Q, validPixROI1, validPixROI2 = cv2.stereoRectify(\n",
        "        cameraMatrix1 = left_K, distCoeffs1 = left_distortion_coefficients,\n",
        "        cameraMatrix2 = right_K, distCoeffs2 = right_distortion_coefficients,\n",
        "        imageSize=imageSize,\n",
        "        newImageSize=newImageSize,\n",
        "        R=R_intercamera, T=T_intercamera,\n",
        "        flags=options['StereoRectify']['flags'] , alpha=alpha)\n",
        "\n",
        "    # Back to Global\n",
        "    R2,T2 = left_R, left_T # perspective is from left image.\n",
        "    R3,T3 = R2.T,numpy.dot(-R2.T,T2) # Invert direction of transformation to map camera to world.\n",
        "    R_left_rectified_to_global = numpy.dot(R3,left_R_rectified.T)\n",
        "    T_left_rectified_to_global = T3\n",
        "    extrinsics_left_rectified_to_global = R_left_rectified_to_global.astype(numpy.float32), T_left_rectified_to_global.astype(numpy.float32)\n",
        "\n",
        "    # Create rectification maps\n",
        "    rectification_map_type = cv2.CV_16SC2\n",
        "    left_maps = cv2.initUndistortRectifyMap(left_K,\n",
        "                                            left_distortion_coefficients,\n",
        "                                            left_R_rectified,\n",
        "                                            P1_rect,\n",
        "                                            size=newImageSize,\n",
        "                                            m1type=rectification_map_type)\n",
        "\n",
        "    right_maps = cv2.initUndistortRectifyMap(right_K,\n",
        "                                            right_distortion_coefficients,\n",
        "                                            right_R_rectified,\n",
        "                                            P2_rect,\n",
        "                                            size=newImageSize,\n",
        "                                            m1type=rectification_map_type)\n",
        "\n",
        "    return Q, extrinsics_left_rectified_to_global, left_maps, right_maps"
      ],
      "metadata": {
        "id": "-kCdabwDhlhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Helper Function: Get the Disparity Matching Algorithm"
      ],
      "metadata": {
        "id": "a2pdI_WJjh8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_disparity_matcher(options):\n",
        "    # Instantiate the matchers; they may do something slow internally...\n",
        "    if 'StereoBM' in options:\n",
        "        # Perform stereo matching using normal block matching\n",
        "        numDisparities = options['StereoMatcher']['NumDisparities']\n",
        "        blockSize = options['StereoMatcher']['BlockSize']\n",
        "        matcher = cv2.StereoBM_create(numDisparities=numDisparities,blockSize=blockSize)\n",
        "        setterOptions = {}\n",
        "        setterOptions.update(options['StereoMatcher'])\n",
        "        setterOptions.update(options['StereoBM'])\n",
        "        for key,value in setterOptions.items():\n",
        "            setter = eval('matcher.set'+key) # Returns the setter function\n",
        "            setter(value) # Calls the setter function.\n",
        "\n",
        "    elif 'StereoSGBM' in options:\n",
        "        # Perform stereo matching using SGBM\n",
        "        minDisparity = options['StereoMatcher']['MinDisparity']\n",
        "        numDisparities = options['StereoMatcher']['NumDisparities']\n",
        "        blockSize = options['StereoMatcher']['BlockSize']\n",
        "        matcher = cv2.StereoSGBM_create(minDisparity=minDisparity,\n",
        "                                    numDisparities=numDisparities,\n",
        "                                    blockSize=blockSize)\n",
        "        setterOptions = {}\n",
        "        setterOptions.update(options['StereoMatcher'])\n",
        "        setterOptions.update(options['StereoSGBM'])\n",
        "        for key,value in setterOptions.items():\n",
        "            setter = eval('matcher.set'+key) # Returns the setter function\n",
        "            setter(value) # Calls the setter function.\n",
        "    else:\n",
        "        assert False, \"Couldn't determine the matcher type from passed options!\"\n",
        "\n",
        "    return matcher"
      ],
      "metadata": {
        "id": "01NKCInQjDHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Class: Call all 3 helper functions"
      ],
      "metadata": {
        "id": "SXh3P2MgjkFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from load_camera_info import load_all_camera_parameters"
      ],
      "metadata": {
        "id": "TaygIf4pk58V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class OpenCVStereoMatcher():\n",
        "    def __init__(self,options=FinalOptions,calibration_path=None):\n",
        "        self.options = options\n",
        "        self.num_cameras = options['CameraArray']['num_cameras']\n",
        "        self.topology = options['CameraArray']['topology']\n",
        "        self.all_camera_parameters = load_all_camera_parameters(calibration_path)\n",
        "\n",
        "        self.left_maps_array = []\n",
        "        self.right_maps_array = []\n",
        "        self.Q_array = []\n",
        "        self.extrinsics_left_rectified_to_global_array = []\n",
        "\n",
        "        for pair_index, (left_index,right_index) in enumerate(topologies[self.topology]):\n",
        "            ## 1 — Get R, T, W, H for each camera\n",
        "            left_K, left_R, left_T, left_width, left_height = [self.all_camera_parameters[left_index][key] for key in ('camera_matrix','R','T','image_width','image_height')]\n",
        "            right_K, right_R, right_T, right_width, right_height = [self.all_camera_parameters[right_index][key] for key in ('camera_matrix','R','T','image_width','image_height')]\n",
        "            h,w = left_height, left_width\n",
        "\n",
        "            # 2 — Stereo Calibrate & Rectify\n",
        "            Q, extrinsics_left_rectified_to_global, left_maps, right_maps = calibrate_and_rectify(options, left_K, right_K,\n",
        "                                                                                                  left_R, right_R,\n",
        "                                                                                                  left_T, right_T)\n",
        "            self.Q_array.append(Q)\n",
        "            self.extrinsics_left_rectified_to_global_array.append(extrinsics_left_rectified_to_global)\n",
        "            self.left_maps_array.append(left_maps)\n",
        "            self.right_maps_array.append(right_maps)\n",
        "\n",
        "            # 3 — Get Matcher\n",
        "            self.matcher = get_disparity_matcher(options)"
      ],
      "metadata": {
        "id": "QshTuztUgBQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imagesPath = Path('data/'+scenario+'/undistorted')\n",
        "workDirectory=Path('.')\n",
        "\n",
        "opencv_matcher = OpenCVStereoMatcher(options=FinalOptions,calibration_path=imagesPath)"
      ],
      "metadata": {
        "id": "4YHJBnnDj_Sm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Print the DIsparity Matcher parameters etc ... to check everything is correct\n",
        "print(opencv_matcher.Q_array[0])"
      ],
      "metadata": {
        "id": "cJYaBirTlKMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3 — Load the Images (part of the class)"
      ],
      "metadata": {
        "id": "HuDWA2f6j3hc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images(self,imagesPath):\n",
        "    # Load a set of images from disk. Doesn't do processing yet.\n",
        "    imagesPath = imagesPath.resolve()\n",
        "\n",
        "    # Load the undistorted images off of disk\n",
        "    print('Loading the images off of disk...')\n",
        "    num_cameras = len(list(imagesPath.glob('*.png')))\n",
        "    assert self.num_cameras == num_cameras, 'Mismatch in the number of available images!'\n",
        "    images = []\n",
        "    for i in range(num_cameras):\n",
        "        fileName = 'image_camera%02i.png' % (i + 1)\n",
        "        filePath = imagesPath / fileName\n",
        "        print('Loading image',filePath)\n",
        "        colorImage = cv2.imread(str(filePath))\n",
        "        grayImage = cv2.cvtColor(colorImage, cv2.COLOR_BGR2GRAY)\n",
        "        images.append(grayImage)\n",
        "        expected_parameters = self.all_camera_parameters[i]\n",
        "        w,h = expected_parameters['image_width'], expected_parameters['image_height']\n",
        "        assert grayImage.shape == (h,w), 'Mismatch in image sizes!'\n",
        "    self.images = images\n",
        "\n",
        "OpenCVStereoMatcher.load_images = load_images"
      ],
      "metadata": {
        "id": "bGL_TMcdj713"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opencv_matcher.load_images(imagesPath)"
      ],
      "metadata": {
        "id": "q7AIYaNylqj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(opencv_matcher.images[0])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wSmrl3fkl2Od"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4 — Test Parameters and Implement Reconstruction\n",
        "\n",
        "The run() function will look like this:\n",
        "1.   For each pair of the topology...\n",
        "2.   Load the two images and rectification maps\n",
        "3.   Rectify\n",
        "4.   Load Disparity & Compute it\n",
        "5.   Reproject to 3D, Postprocess, Save"
      ],
      "metadata": {
        "id": "uLhHz04r_JN_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "StereoMatcherOptions = {'MinDisparity': -64, # Influences MAX depth\n",
        "                        'NumDisparities': 256, # Influences MIN depth\n",
        "                        'BlockSize': 21,\n",
        "                        'SpeckleWindowSize': 0, # Must be strictly positive to turn on speckle post-filter.\n",
        "                        'SpeckleRange': 0, # Must be >= 0 to enable speckle post-filter\n",
        "                        'Disp12MaxDiff': 0}\n",
        "\n",
        "StereoBMOptions = {\n",
        "        'PreFilterType': (cv2.StereoBM_PREFILTER_NORMALIZED_RESPONSE, cv2.StereoBM_PREFILTER_XSOBEL)[0],\n",
        "                   'PreFilterSize': 5, # preFilterSize must be odd and be within 5..255\n",
        "                   'PreFilterCap': 63, # preFilterCap must be within 1..63. Used to truncate pixel values\n",
        "                   'TextureThreshold': 10,\n",
        "                   'UniquenessRatio': 10,\n",
        "                   }\n",
        "\n",
        "StereoSGBMOptions = {'PreFilterCap': 0,\n",
        "                     'UniquenessRatio': 0,\n",
        "                     'P1': 16*21*21, # \"Depth Change Cost in Ensenso terminology\"\n",
        "                     'P2': 16*21*21, # \"Depth Step Cost in Ensenso terminology\"\n",
        "                     'Mode': (cv2.StereoSGBM_MODE_SGBM, cv2.StereoSGBM_MODE_HH,\n",
        "                              cv2.StereoSGBM_MODE_SGBM_3WAY)[1]}\n",
        "\n",
        "StereoRectifyOptions = {'imageSize':(w,h),\n",
        "                        'flags':(0,cv2.CALIB_ZERO_DISPARITY)[0], # TODO explore other flags\n",
        "                        'newImageSize':(w,h),\n",
        "                        'alpha':0.5}\n",
        "\n",
        "RemapOptions = {'interpolation':cv2.INTER_LINEAR}\n",
        "\n",
        "CameraArrayOptions = {'channels':1,'num_cameras':12,'topology':'overlapping'} #topology=skipping_1, skipping_2, adjacent, overlapping, 360\n",
        "\n",
        "FinalOptions = {'StereoRectify':StereoRectifyOptions,\n",
        "        'StereoMatcher':StereoMatcherOptions,\n",
        "        'StereoBM':StereoBMOptions,#change to \"StereoSGBM\" = StereoSGBMOptions if needed\n",
        "        'CameraArray':CameraArrayOptions,\n",
        "        'Remap':RemapOptions}"
      ],
      "metadata": {
        "id": "PPsjtD8CGLLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 1. Rectification\n",
        "First, we're going to see if our **rectification** is correct.\n",
        "\n",
        "The process for that is as follows:\n",
        "1.   Load 2 images & Visualize them\n",
        "2.   Load the rectification map\n",
        "3.   Rectify & Show result"
      ],
      "metadata": {
        "id": "ps2QXPIc_mDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "opencv_matcher = OpenCVStereoMatcher(options=FinalOptions,calibration_path=imagesPath)\n",
        "opencv_matcher.load_images(imagesPath)\n",
        "\n",
        "# Get the Images\n",
        "img0 = opencv_matcher.images[0]\n",
        "img1 = opencv_matcher.images[1]\n",
        "\n",
        "# Get the Maps\n",
        "left_maps = opencv_matcher.left_maps_array[0]\n",
        "right_maps = opencv_matcher.right_maps_array[0]\n",
        "\n",
        "# Rectify with the parameters\n",
        "remap_int = opencv_matcher.options['Remap']['interpolation']\n",
        "left_image_rectified = cv2.remap(img0, left_maps[0],left_maps[1], remap_int)\n",
        "right_image_rectified = cv2.remap(img1, right_maps[0], right_maps[1], remap_int)\n",
        "\n",
        "# Show Results\n",
        "f, (f0, f1, f2) = plt.subplots(1,3,figsize=(20,10))\n",
        "f0.imshow(img0)\n",
        "f1.imshow(left_maps[1])\n",
        "f2.imshow(left_image_rectified)\n",
        "f, (f3, f4, f5) = plt.subplots(1,3,figsize=(20,10))\n",
        "f3.imshow(img1)\n",
        "f4.imshow(right_maps[1])\n",
        "f5.imshow(right_image_rectified)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CDWirDN-b6aY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(abs(left_image_rectified - right_image_rectified));"
      ],
      "metadata": {
        "id": "U_OusPN9Hcwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 2: Disparity Estimation\n",
        "Now, we're going to play with the disparity and see if we can do something interesting. This time, and to simplify the code, **we won't do it for every single image of the topology, just 2**, and we'll generalize in our final code.\n",
        "\n",
        "We're just goint to grab the last 2 images we tried our code on. — make sure your topology allows for disparity estimate."
      ],
      "metadata": {
        "id": "1t1sRYREBp-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find Disparity\n",
        "matcher = opencv_matcher.matcher\n",
        "disparity_img = matcher.compute(left_image_rectified, right_image_rectified)\n",
        "\n",
        "if disparity_img.dtype == numpy.int16:\n",
        "    disparity_img = disparity_img.astype(numpy.float32)\n",
        "    disparity_img /= 16\n",
        "\n",
        "# Show Results\n",
        "plt.imshow(disparity_img)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "h186XIsWK1c2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2Bis — Find the right parameters"
      ],
      "metadata": {
        "id": "m-UxLPJ_Tmus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ipywidgets import interact, interactive, fixed\n",
        "\n",
        "def compute_disparity(image, img_pair, num_disparities=6*16, block_size=11, window_size=6, uniqueness_ratio=0, speckleWindowSize = 200, matcher=\"stereo_sgbm\", show_disparity=True):\n",
        "    if matcher == \"stereo_bm\":\n",
        "        new_image = cv2.StereoBM_create(numDisparities=num_disparities,blockSize=block_size)\n",
        "        new_image.setPreFilterType(1)\n",
        "        new_image.setUniquenessRatio(uniqueness_ratio)\n",
        "        new_image.setSpeckleRange(2)\n",
        "        new_image.setSpeckleWindowSize(speckleWindowSize)\n",
        "    elif matcher == \"stereo_sgbm\":\n",
        "        new_image = cv2.StereoSGBM_create(minDisparity=0, numDisparities=num_disparities, blockSize=block_size,\n",
        "                                         uniquenessRatio=uniqueness_ratio, speckleWindowSize=speckleWindowSize, speckleRange=2, disp12MaxDiff=1,\n",
        "                                         P1=8 * 1 * window_size **2, P2=32 * 1 * window_size **2)\n",
        "\n",
        "    new_image = new_image.compute(image, img_pair).astype(numpy.float32)/16\n",
        "\n",
        "    if (show_disparity==True):\n",
        "        plt.figure(figsize = (20,10))\n",
        "        plt.imshow(new_image, cmap=\"plasma\")\n",
        "        plt.show()\n",
        "    return new_image\n"
      ],
      "metadata": {
        "id": "KE_gDoSvK8Ax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_d = (0,512,16)\n",
        "b_s = (1,31,2)\n",
        "window_s = (1,13,2)\n",
        "uniqueness_r = (0, 10, 1)\n",
        "speckle_w = (0, 250, 50)\n",
        "\n",
        "disparity_left = interactive(compute_disparity, image=fixed(left_image_rectified), img_pair = fixed(right_image_rectified), num_disparities=num_d, block_size=b_s, window_size=window_s, matcher=[\"stereo_sgbm\", \"stereo_bm\"], uniqueness_ratio= uniqueness_r, speckleWindowSize=speckle_w)\n",
        "display(disparity_left)"
      ],
      "metadata": {
        "id": "NjMp77RyLIRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you have good parameters, be ready to recompute the **FinalOptions** variable"
      ],
      "metadata": {
        "id": "Fd-dFvfMLr2Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2BisBis — CreSTEREO Disparity"
      ],
      "metadata": {
        "id": "m-h9g7awqda4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ibaiGorordo/ONNX-CREStereo-Depth-Estimation\n",
        "! pip3 install -r ONNX-CREStereo-Depth-Estimation/requirements.txt\n",
        "!pip3 install onnxruntime-gpu\n",
        "import os\n",
        "os.chdir(\"ONNX-CREStereo-Depth-Estimation\")\n",
        "from crestereo import CREStereo\n",
        "iters = 20            # Lower iterations are faster, but will lower detail.\n",
        "\t\t             # Options: 2, 5, 10, 20\n",
        "\n",
        "shape = (480, 640)   # Input resolution.\n",
        "\t\t\t\t     # Options: (240,320), (320,480), (380, 480), (360, 640), (480,640), (720, 1280)\n",
        "\n",
        "version = \"combined\" # The combined version does 2 passes, one to get an initial estimation and a second one to refine it.\n",
        "\t\t\t\t\t # Options: \"init\", \"combined\"\n",
        "\n",
        "!git clone https://github.com/PINTO0309/PINTO_model_zoo.git\n",
        "!bash PINTO_model_zoo/284_CREStereo/download_iter20.sh\n",
        "!ls\n",
        "!cp -r crestereo_*.onnx models/\n",
        "# Initialize model\n",
        "model_path = f'models/crestereo_{version}_iter{20}_{shape[0]}x{shape[1]}.onnx'\n",
        "depth_estimator = CREStereo(model_path)"
      ],
      "metadata": {
        "id": "9mNaVCNOqgL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disparity_img = depth_estimator(left_image_rectified,right_image_rectified)\n",
        "plt.imshow(disparity_img)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yGhkkINnsADn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disparity_img.min(), disparity_img.max()"
      ],
      "metadata": {
        "id": "yxQ3pjZmK1dR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(disparity_img >= 100)"
      ],
      "metadata": {
        "id": "8VYhJBt9K8tM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 3: Project to 3D\n",
        "Finally, we'll see the code to project into 3D.\n",
        "This code will mainly project the Disparity into the Q matrix."
      ],
      "metadata": {
        "id": "cN4uoyUSMTjQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reproject 3D\n",
        "Q = opencv_matcher.Q_array[0]\n",
        "threedeeimage = cv2.reprojectImageTo3D(disparity_img, Q, handleMissingValues=True,ddepth=cv2.CV_32F)\n",
        "threedeeimage = numpy.array(threedeeimage)\n",
        "\n",
        "# Postprocess\n",
        "xyz = threedeeimage.reshape((-1,3)) # x,y,z now in three columns, in left rectified camera coordinates\n",
        "z = xyz[:,2]\n",
        "goodz = z < 9999.0\n",
        "xyz_filtered = xyz[goodz,:]\n",
        "\n",
        "# Global Coordinates\n",
        "R_left_rectified_to_global, T_left_rectified_to_global = opencv_matcher.extrinsics_left_rectified_to_global_array[0]\n",
        "xyz_global_ = numpy.dot(xyz_filtered, R_left_rectified_to_global.T) + T_left_rectified_to_global.T\n",
        "\n",
        "# Save PLY\n",
        "save_ply(xyz_global_, 'pair_test_0_1.ply')"
      ],
      "metadata": {
        "id": "UdykTgNWMcHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyntcloud\n",
        "from pyntcloud import PyntCloud"
      ],
      "metadata": {
        "id": "_ejLsPFOPEx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "object_3d = PyntCloud.from_file(\"pair_test_0_1.ply\")\n",
        "object_3d.plot()"
      ],
      "metadata": {
        "id": "kTAo9xlWPEKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5 — Putting it all together"
      ],
      "metadata": {
        "id": "5_8xnC3nl7Ro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run(self):\n",
        "    assert self.all_camera_parameters is not None, 'Camera parameters not loaded yet; You should run load_all_camera_parameters first!'\n",
        "    xyz_global_array = [None]*len(topologies[self.topology])\n",
        "\n",
        "    def run_pair(pair_idx, left_idx, right_idx):\n",
        "        # Load the proper images and rectification maps\n",
        "        left_img, right_img = self.images[left_idx], self.images[right_idx]\n",
        "        left_maps = self.left_maps_array[pair_idx]\n",
        "        right_maps = self.right_maps_array[pair_idx]\n",
        "\n",
        "        # Rectify\n",
        "        remap_interpolation = self.options['Remap']['interpolation']\n",
        "        left_image_rectified = cv2.remap(left_img, left_maps[0],left_maps[1], remap_interpolation)\n",
        "        right_image_rectified = cv2.remap(right_img, right_maps[0], right_maps[1], remap_interpolation)\n",
        "\n",
        "        # Load & Find Disparity\n",
        "        disparity_image = self.matcher.compute(left_image_rectified, right_image_rectified)\n",
        "\n",
        "        if disparity_image.dtype == numpy.int16:\n",
        "            disparity_image = disparity_image.astype(numpy.float32)\n",
        "            disparity_image /= 16\n",
        "\n",
        "        plt.imshow(disparity_image)\n",
        "        plt.show()\n",
        "\n",
        "        # Reproject 3D\n",
        "        Q = self.Q_array[pair_idx]\n",
        "        threedeeimage = cv2.reprojectImageTo3D(disparity_image, Q, handleMissingValues=True,ddepth=cv2.CV_32F)\n",
        "        threedeeimage = numpy.array(threedeeimage)\n",
        "\n",
        "        # Postprocess\n",
        "        xyz = threedeeimage.reshape((-1,3)) # x,y,z now in three columns, in left rectified camera coordinates\n",
        "        z = xyz[:,2]\n",
        "        goodz = z < 9999.0\n",
        "        xyz_filtered = xyz[goodz,:]\n",
        "\n",
        "        # Global Coordinates\n",
        "        R_left_rectified_to_global, T_left_rectified_to_global = self.extrinsics_left_rectified_to_global_array[pair_idx]\n",
        "        xyz_global = numpy.dot(xyz_filtered, R_left_rectified_to_global.T) + T_left_rectified_to_global.T\n",
        "\n",
        "        # Save PLY\n",
        "        save_ply(xyz_global, 'pair_'+str(left_index)+'_'+str(right_index)+'.ply')\n",
        "        xyz_global_array[pair_index] = xyz_global\n",
        "\n",
        "    for pair_index, (left_index,right_index) in enumerate(topologies[self.topology]):\n",
        "        run_pair(pair_index, left_index, right_index)\n",
        "\n",
        "    xyz = numpy.vstack(xyz_global_array)\n",
        "    return xyz\n",
        "\n",
        "OpenCVStereoMatcher.run = run"
      ],
      "metadata": {
        "id": "zIajEloE7IYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main"
      ],
      "metadata": {
        "id": "A3oUB-YR9c5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imagesPath = Path('/content/data/'+scenario+'/undistorted')\n",
        "workDirectory=Path('.')\n",
        "\n",
        "opencv_matcher = OpenCVStereoMatcher(options=FinalOptions, calibration_path=imagesPath)\n",
        "opencv_matcher.load_images(imagesPath)\n",
        "xyz = opencv_matcher.run()\n",
        "\n",
        "save_ply(xyz, \"therock.ply\")"
      ],
      "metadata": {
        "id": "Ecq331MM9dvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "id": "XdysLsPQMadV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "object_3d = PyntCloud.from_file(\"therock.ply\")\n",
        "object_3d.plot()"
      ],
      "metadata": {
        "id": "3q8sRvCMQCiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Temple Reconstruction] The Temple is Under Attack!\n",
        "\n",
        "![](https://vision.middlebury.edu/mview/data/images/temple0119.jpg)\n",
        "\n",
        "Dear warrior,\n",
        "\n",
        "**Our beloved temple is under attack.**\n",
        "\n",
        "The enemy is at the door, and they're going to kill the younglings.\n",
        "We can hold the siege, but soon or later, it's going to fall.\n",
        "\n",
        "Only you can preserve its memory.\n",
        "\n",
        "We took 12 pictures of this temple, and saved the camera information in a folder named \"temple\".\n",
        "\n",
        "**Everything is done, but we need to adjust the Stereo Vision parameters.**\n",
        "\n",
        "And only you can do it.\n",
        "\n",
        "Help us, you're our only hope.\n",
        "\n",
        "<p>\n",
        "\n",
        "Below are a few functions to help you with the task!"
      ],
      "metadata": {
        "id": "TKjjm6gjYkFx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. First, the **\"load camera parameters\"** function has been modified to handle the new dataset. It's already imported with the wget function you ran (cell #1 of this notebook):"
      ],
      "metadata": {
        "id": "XbyLCUiKZbXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from load_camera_info_temple import load_all_camera_parameters_temple"
      ],
      "metadata": {
        "id": "Aaodav7tYtzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Then, you'll need to load the correct images, with the right scenario."
      ],
      "metadata": {
        "id": "jkcNj_GWZnq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scenario =\"temple\""
      ],
      "metadata": {
        "id": "VDmubt3CY49j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####3. These are some parameters that have been implemented by the team working on Temple Reconstruction in C++. Here is the original repo where you can grab TONS of cool ideas: https://github.com/temple-reconstruction/mview/blob/master/pure-cv/main.cpp"
      ],
      "metadata": {
        "id": "VqV9o2GWZrAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "StereoMatcherTemple = {\n",
        "                        'MinDisparity': 0,\n",
        "                        'NumDisparities': 64,\n",
        "                        'BlockSize': 7,\n",
        "                        'Disp12MaxDiff': 0,\n",
        "                        'PreFilterCap' : 0,\n",
        "                        'UniquenessRatio' : 15,\n",
        "                        'SpeckleWindowSize' : 50,\n",
        "                        'SpeckleRange' : 1\n",
        "                     }\n",
        "\n",
        "StereoSGBMTemple = {\n",
        "    'PreFilterCap': 0,\n",
        "    'UniquenessRatio': 0,\n",
        "    'P1': 8, # \"Depth Change Cost in Ensenso terminology\"\n",
        "    'P2': 32, # \"Depth Step Cost in Ensenso terminology\"\n",
        "    }\n",
        "\n",
        "TempleOptions = {'StereoRectify':StereoRectifyOptions,\n",
        "        'StereoMatcher':StereoMatcherTemple,\n",
        "        'StereoSGBM':StereoSGBMTemple,#change to \"StereoSGBM\" = StereoSGBMOptions if needed\n",
        "        'CameraArray':CameraArrayOptions,\n",
        "        'Remap':RemapOptions}"
      ],
      "metadata": {
        "id": "yWr-AzDtZAG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####4. In the notebook, we have a get_disparity() function — This one is doing it with the new options and implements StereoSGBM instead."
      ],
      "metadata": {
        "id": "Wcw6E5ADZyuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_disparity_temple(options):\n",
        "    minDisparity = options['StereoMatcher']['MinDisparity']\n",
        "    numDisparities = options['StereoMatcher']['NumDisparities']\n",
        "    blockSize = options['StereoMatcher']['BlockSize']\n",
        "    P1 = options['StereoSGBM']['P1']\n",
        "    P2 = options['StereoSGBM']['P2']\n",
        "    Disp12MaxDiff = options['StereoMatcher']['Disp12MaxDiff']\n",
        "    uniquenessRatio = options['StereoSGBM']['UniquenessRatio']\n",
        "    speckleWindowSize = options['StereoMatcher']['SpeckleWindowSize']\n",
        "    speckleRange = options['StereoMatcher']['SpeckleRange']\n",
        "    PreFilterCap = options['StereoSGBM']['PreFilterCap']\n",
        "\n",
        "    matcher = cv2.StereoSGBM_create(\n",
        "                                    minDisparity=minDisparity,\n",
        "                                    numDisparities=numDisparities,\n",
        "                                    blockSize=blockSize,\n",
        "                                    P1=P1,\n",
        "                                    P2=P2,\n",
        "                                    disp12MaxDiff = Disp12MaxDiff,\n",
        "                                    preFilterCap =PreFilterCap,\n",
        "                                    uniquenessRatio=uniquenessRatio,\n",
        "                                    speckleWindowSize=speckleWindowSize,\n",
        "                                    speckleRange=speckleRange)\n",
        "    setterOptions = {}\n",
        "    setterOptions.update(options['StereoMatcher'])\n",
        "    setterOptions.update(options['StereoSGBM'])\n",
        "\n",
        "    #setterOptions.update(options['StereoSGBM'])\n",
        "    for key,value in setterOptions.items():\n",
        "        setter = eval('matcher.set'+key) # Returns the setter function\n",
        "        setter(value) # Calls the setter function.\n",
        "\n",
        "    return matcher"
      ],
      "metadata": {
        "id": "FpcuYLzHZD0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####5. Following the C++ folks, this function implements disparity estimation with StereoSGBM but also adds something called a WLS filter.\n",
        "\n",
        "Here is an image without the WLS filter:\n",
        "![](https://docs.opencv.org/3.4/ambush_5_bm.png)\n",
        "And one with the filter:\n",
        "![](https://docs.opencv.org/3.4/ambush_5_bm_with_filter.png)\n"
      ],
      "metadata": {
        "id": "Pwo2KDUsZ6Rk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_disparity_temple_filter(rimg1, rimg2):\n",
        "    maxd = 2\n",
        "    window_size = 5\n",
        "    left_matcher = cv2.StereoSGBM_create(minDisparity=-maxd, numDisparities=11,\n",
        "        blockSize=5,\n",
        "        P1=8 * 3 * window_size ** 2,\n",
        "        P2=32 * 3 * window_size ** 2,\n",
        "        disp12MaxDiff=1,\n",
        "        uniquenessRatio=15,\n",
        "        speckleWindowSize=0,\n",
        "        speckleRange=2,\n",
        "        preFilterCap=63,\n",
        "        mode=cv2.STEREO_SGBM_MODE_SGBM_3WAY\n",
        "    )\n",
        "    right_matcher = cv2.ximgproc.createRightMatcher(left_matcher)\n",
        "    lmbda = 8000\n",
        "    sigma = 1.5\n",
        "    wls_filter = cv2.ximgproc.createDisparityWLSFilter(matcher_left=left_matcher)\n",
        "    wls_filter.setLambda(lmbda)\n",
        "    wls_filter.setSigmaColor(sigma)\n",
        "    displ = left_matcher.compute(rimg1, rimg2)\n",
        "    dispr = right_matcher.compute(rimg2, rimg1)\n",
        "    displ = numpy.int16(displ)\n",
        "    dispr = numpy.int16(dispr)\n",
        "    disparity = wls_filter.filter(displ, rimg1, None, dispr) / 16.0\n",
        "    return disparity"
      ],
      "metadata": {
        "id": "JYzxHteJZKJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####6. In the code, you'll need to call the right disparity function."
      ],
      "metadata": {
        "id": "HaQH8B2CaKyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imagesPath = Path('data/'+scenario+'/undistorted')\n",
        "\n",
        "opencv_matcher = OpenCVStereoMatcher(options=TempleOptions,calibration_path=imagesPath)\n",
        "opencv_matcher.load_images(imagesPath)\n",
        "\n",
        "# Get the Images\n",
        "img0 = opencv_matcher.images[0]\n",
        "img1 = opencv_matcher.images[1]\n",
        "\n",
        "# Get the Maps\n",
        "left_maps = opencv_matcher.left_maps_array[0]\n",
        "right_maps = opencv_matcher.right_maps_array[0]\n",
        "\n",
        "# Rectify with the parameters\n",
        "remap_int = opencv_matcher.options['Remap']['interpolation']\n",
        "left_image_rectified = cv2.remap(img0, left_maps[0],left_maps[1], remap_int)\n",
        "right_image_rectified = cv2.remap(img1, right_maps[0], right_maps[1], remap_int)"
      ],
      "metadata": {
        "id": "J2sBy6HXNr-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show Results\n",
        "f, (f0, f1, f2) = plt.subplots(1,3,figsize=(20,10))\n",
        "f0.imshow(img0)\n",
        "f1.imshow(left_maps[1])\n",
        "f2.imshow(left_image_rectified)\n",
        "f, (f3, f4, f5) = plt.subplots(1,3,figsize=(20,10))\n",
        "f3.imshow(img1)\n",
        "f4.imshow(right_maps[1])\n",
        "f5.imshow(right_image_rectified)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fP0YT76MOt4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alright, good luck. If you get good results, come to me with the solutions. I'll have something cool for you..."
      ],
      "metadata": {
        "id": "SS5OV28HaOfv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "disparity_img = get_disparity_temple_filter(left_image_rectified, right_image_rectified).astype(numpy.float32)/16"
      ],
      "metadata": {
        "id": "kBiIXE8WMwkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(img0);"
      ],
      "metadata": {
        "id": "Rz4oEE7sN4hQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(img1)"
      ],
      "metadata": {
        "id": "6s5QJYeCOiAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "left_image_rectified.min(), left_image_rectified.max()"
      ],
      "metadata": {
        "id": "5P9Wd4kaObVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(left_image_rectified)"
      ],
      "metadata": {
        "id": "1DtWfnpQN6PW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ip4Ca5wBN9KH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}