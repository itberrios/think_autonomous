{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RAFT Starter.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMA3ldiTHkL0XCvWoVirEyq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jeremy26/optical_flow_course/blob/main/RAFT_Starter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVdTJsytfFL9"
      },
      "source": [
        "# RAFT\n",
        "\n",
        "Let's run a RAFT optical flow algorithm\n",
        "<p>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czK46V1BZH4K"
      },
      "source": [
        "!git clone https://github.com/princeton-vl/RAFT.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLVN_ODAZV5t"
      },
      "source": [
        "!ls RAFT"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEWfWfLG6lFF"
      },
      "source": [
        "!./RAFT/download_models.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBqcqasaeBzn"
      },
      "source": [
        "!wget https://thinkautonomous-raft.s3.eu-west-3.amazonaws.com/raft_data.zip && unzip raft_data.zip && rm raft_data.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca68eN0UkJ6B"
      },
      "source": [
        "!mv raft.py RAFT/core/raft.py "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3SXmoALkRr5"
      },
      "source": [
        "!mv update.py RAFT/core/update.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RWo_aivW0Tn"
      },
      "source": [
        "# Run RAFT on 2 images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2P4eAMEUHrY"
      },
      "source": [
        "import sys\n",
        "sys.path.append('RAFT/core')\n",
        "from raft import RAFT\n",
        "from utils import flow_viz\n",
        "from utils.utils import InputPadder\n",
        "from collections import OrderedDict\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from utils import flow_viz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuYDzMfpnSjR"
      },
      "source": [
        "def bgr2rgb(img):\n",
        "    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaMWXY9gXxHe"
      },
      "source": [
        "def frame_preprocess(frame, device):\n",
        "    frame = torch.from_numpy(frame).permute(2, 0, 1).float()\n",
        "    frame = frame.unsqueeze(0)\n",
        "    frame = frame.to(device)\n",
        "    return frame"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5xB7cdVX7gW"
      },
      "source": [
        "def get_cpu_model(model):\n",
        "    new_model = OrderedDict()\n",
        "    # get all layer's names from model\n",
        "    for name in model:\n",
        "        # create new name and update new model\n",
        "        new_name = name[7:]\n",
        "        new_model[new_name] = model[name]\n",
        "    return new_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3Y9FJOQoFgZ"
      },
      "source": [
        "def load_model(weights_path):\n",
        "    model = RAFT()\n",
        "    pretrained_weights = torch.load(weights_path, map_location=torch.device(\"cpu\"))\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda\"\n",
        "        # parallel between available GPUs\n",
        "        model = torch.nn.DataParallel(model)\n",
        "        # load the pretrained weights into model\n",
        "        model.load_state_dict(pretrained_weights)\n",
        "        model.to(device)\n",
        "    else:\n",
        "        device = \"cpu\"\n",
        "        # change key names for CPU runtime\n",
        "        pretrained_weights = get_cpu_model(pretrained_weights)\n",
        "        # load the pretrained weights into model\n",
        "        model.load_state_dict(pretrained_weights)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IroszXk6kgEQ"
      },
      "source": [
        "def inference_imgs(model, frame_1, frame_2):\n",
        "    \n",
        "    # change model's mode to evaluation\n",
        "    model.eval()\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Read images\n",
        "        frame_1 = frame_preprocess(frame_1, device)\n",
        "        frame_2 = frame_preprocess(frame_2, device)\n",
        "        # preprocessing\n",
        "        padder = InputPadder(frame_1.shape, mode=\"kitti\")\n",
        "        frame_1, frame_2 = padder.pad(frame_1, frame_2)\n",
        "\n",
        "        # predict the flow\n",
        "        flow_low, flow_up = model(frame_1, frame_2, iters=12, test_mode=True)\n",
        "\n",
        "        # transform to image\n",
        "        flo = flow_up[0].permute(1,2,0).cpu().numpy()\n",
        "        flo = flow_viz.flow_to_image(flo)\n",
        "    return flow_up, flo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cP9cheJ5lYQI"
      },
      "source": [
        "img_1 = cv2.imread(\"raft_data/0000000148.png\")\n",
        "img_2 = cv2.imread(\"raft_data/000000149.png\")\n",
        "model = load_model(\"models/raft-kitti.pth\")\n",
        "\n",
        "flow_up, flo = inference_imgs(model, img_1, img_2)\n",
        "\n",
        "f, (ax0, ax1) = plt.subplots(1,2, figsize=(20,10))\n",
        "ax0.imshow(bgr2rgb(img_1))\n",
        "ax1.imshow(flo)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yG5VRUncl3nR"
      },
      "source": [
        "# Understand the Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySQ0uDjSARqs"
      },
      "source": [
        "print(flow_up.shape)\n",
        "print(flow_up[0][0])\n",
        "print(flow_up[0][0].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sPvie43a0HR"
      },
      "source": [
        "# Run an Object Detection algorithm to identify individual objects"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_Y_zDwd86x2"
      },
      "source": [
        "!python3 -m pip install yolov4==2.0.2 # After Checking, YOLO 2.0.2 works without modifying anything. Otherwise keep 1.2.1\n",
        "from yolov4.tf import YOLOv4\n",
        "import tensorflow as tf\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YjkA7M7fGYX"
      },
      "source": [
        "yolo = YOLOv4(tiny=True)\n",
        "yolo.classes = \"raft_data/coco.names\"\n",
        "yolo.make_model()\n",
        "yolo.load_weights(\"raft_data/yolov4-tiny.weights\", weights_type=\"yolo\")\n",
        "\n",
        "def run_obstacle_detection(img):\n",
        "    start_time=time.time()\n",
        "    #img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    resized_image = yolo.resize_image(img)\n",
        "    # 0 ~ 255 to 0.0 ~ 1.0\n",
        "    resized_image = resized_image / 255.\n",
        "    #input_data == Dim(1, input_size, input_size, channels)\n",
        "    input_data = resized_image[np.newaxis, ...].astype(np.float32)\n",
        "\n",
        "    candidates = yolo.model.predict(input_data)\n",
        "\n",
        "    _candidates = []\n",
        "    result = img.copy()\n",
        "    for candidate in candidates:\n",
        "        batch_size = candidate.shape[0]\n",
        "        grid_size = candidate.shape[1]\n",
        "        _candidates.append(tf.reshape(candidate, shape=(1, grid_size * grid_size * 3, -1)))\n",
        "        #candidates == Dim(batch, candidates, (bbox))\n",
        "        candidates = np.concatenate(_candidates, axis=1)\n",
        "        #pred_bboxes == Dim(candidates, (x, y, w, h, class_id, prob))\n",
        "        pred_bboxes = yolo.candidates_to_pred_bboxes(candidates[0], iou_threshold=0.35, score_threshold=0.40)\n",
        "        pred_bboxes = pred_bboxes[~(pred_bboxes==0).all(1)] #https://stackoverflow.com/questions/35673095/python-how-to-eliminate-all-the-zero-rows-from-a-matrix-in-numpy?lq=1\n",
        "        pred_bboxes = yolo.fit_pred_bboxes_to_original(pred_bboxes, img.shape)\n",
        "        exec_time = time.time() - start_time\n",
        "        #print(\"time: {:.2f} ms\".format(exec_time * 1000))\n",
        "        result = yolo.draw_bboxes(img, pred_bboxes)\n",
        "    return result, pred_bboxes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AM0OEQp1IusZ"
      },
      "source": [
        "result, pred_bboxes = run_obstacle_detection(bgr2rgb(img_1))\n",
        "\n",
        "plt.imshow(result)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elmdRVy_mq2E"
      },
      "source": [
        "# Evaluate the Motion of each obstacle through time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8UmtZSG9Tyz"
      },
      "source": [
        "def add_arrow_to_box(result, pred_bboxes, fl_vectors):\n",
        "    h, w, _ = result.shape\n",
        "    image_arr = []\n",
        "    \n",
        "    #For each box, add an arrow that shows the flow of each obstacle\n",
        "    return image_arr\n",
        "\n",
        "image_arr = add_arrow_to_box(result, pred_bboxes, flow_up)\n",
        "plt.imshow(image_arr)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgzBB8sSdty7"
      },
      "source": [
        "f, (ax0, ax1)= plt.subplots(1, 2, figsize=(20,10))\n",
        "ax0.imshow(image_arr)\n",
        "ax1.imshow(flo)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giNVtchwnt_e"
      },
      "source": [
        "print(flo.shape)\n",
        "print(image_arr.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-kIprevnneF"
      },
      "source": [
        "# Run on a Video 🙌🏼"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxT0L_PRn4hp"
      },
      "source": [
        "def inference_video(video_path):\n",
        "    model = load_model(\"models/raft-kitti.pth\")\n",
        "    # change model's mode to evaluation\n",
        "    model.eval()\n",
        "    # capture the video and get the first frame\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    ret, cap1 = cap.read()\n",
        "    video_frames_arrow = []\n",
        "    video_frames_flow = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        while True:\n",
        "            # read the next frame\n",
        "            ret, cap2 = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            # Predict the Flow\n",
        "            flow_up, flo = inference_imgs(model, cap1.copy(), cap2.copy())\n",
        "            # Run obstacle Detection\n",
        "            result, pred_bboxes = run_obstacle_detection(bgr2rgb(cap2))\n",
        "            # Add Motion Prediction\n",
        "            image_arr = bgr2rgb(add_arrow_to_box(result, pred_bboxes, flow_up))\n",
        "            video_frames_arrow.append(image_arr)\n",
        "            video_frames_flow.append(flo)\n",
        "            # mode forward one frame\n",
        "            cap1 = cap2\n",
        "    return video_frames_arrow, video_frames_flow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHwEI_koqRuh"
      },
      "source": [
        "video_frames_arrow, video_frames_flow = inference_video(\"raft_data/kitti_3.mp4\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pzALEB_qZp-"
      },
      "source": [
        "out = cv2.VideoWriter(\"output_flow.mp4\",cv2.VideoWriter_fourcc(*'mp4v'), 15.0, (video_frames_flow[0].shape[1] ,video_frames_flow[0].shape[0]))\n",
        "for i in range(len(video_frames_flow)):\n",
        "    out.write(video_frames_flow[i].astype(np.uint8))\n",
        "out.release()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyoYeIykqHT4"
      },
      "source": [
        "out = cv2.VideoWriter(\"output_arrow.mp4\",cv2.VideoWriter_fourcc(*'mp4v'), 15.0, (video_frames_arrow[0].shape[1] ,video_frames_arrow[0].shape[0]))\n",
        "for i in range(len(video_frames_arrow)):\n",
        "    out.write(video_frames_arrow[i].astype(np.uint8))\n",
        "out.release()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jV4-vEKuwTEF"
      },
      "source": [
        "# Legacy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69LX9JdkFPJh"
      },
      "source": [
        "def inference():\n",
        "    # Outputs to return\n",
        "    result_flows_vectors = []\n",
        "    result_flows_images = []\n",
        "    images = []\n",
        "\n",
        "    # Get the RAFT model\n",
        "    model = RAFT()\n",
        "\n",
        "    # Load pretrained weights\n",
        "    pretrained_weights = torch.load(\"models/raft-kitti.pth\", map_location=torch.device(\"cpu\"))\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda\"\n",
        "        # parallel between available GPUs\n",
        "        model = torch.nn.DataParallel(model)\n",
        "        # load the pretrained weights into model\n",
        "        model.load_state_dict(pretrained_weights)\n",
        "        model.to(device)\n",
        "    else:\n",
        "        device = \"cpu\"\n",
        "        # change key names for CPU runtime\n",
        "        pretrained_weights = get_cpu_model(pretrained_weights)\n",
        "        # load the pretrained weights into model\n",
        "        model.load_state_dict(pretrained_weights)\n",
        "\n",
        "    # change model's mode to evaluation\n",
        "    model.eval()\n",
        "\n",
        "    video_path = \"kitti_3.mp4\"\n",
        "\n",
        "    # capture the video and get the first frame\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    ret, frame_1 = cap.read()\n",
        "\n",
        "    # Save the image\n",
        "    images.append(cv2.cvtColor(frame_1, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "    # frame preprocessing\n",
        "    frame_1 = frame_preprocess(frame_1, device)\n",
        "\n",
        "    counter = 0\n",
        "    with torch.no_grad():\n",
        "        while True:\n",
        "            # read the next frame\n",
        "            ret, frame_2_b = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            # save the image\n",
        "            images.append(cv2.cvtColor(frame_2_b, cv2.COLOR_BGR2RGB))\n",
        "            # preprocessing\n",
        "            frame_2_b = frame_preprocess(frame_2_b, device)\n",
        "            padder = InputPadder(frame_1.shape, mode=\"kitti\")\n",
        "            frame_1, frame_2 = padder.pad(frame_1, frame_2_b)\n",
        "            # predict the flow\n",
        "            flow_low, flow_up = model(frame_1, frame_2, iters=12, test_mode=True)\n",
        "            # save the flow\n",
        "            result_flows_vectors.append(flow_up.cpu().detach().numpy())\n",
        "            # transform to image\n",
        "            flo = flow_up[0].permute(1,2,0).cpu().numpy()\n",
        "            flo = flow_viz.flow_to_image(flo)\n",
        "            # save the image\n",
        "            result_flows_images.append(flo)\n",
        "            # mode forward one frame\n",
        "            frame_1 = frame_2_b    \n",
        "    return result_flows_vectors, result_flows_images, images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkTqOanXVhHN"
      },
      "source": [
        "fl_vectors, fl_images, images = inference()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_GOfA-5WfV1"
      },
      "source": [
        "final_vid = []\n",
        "\n",
        "for idx, img in enumerate(images):\n",
        "    if idx != 0:\n",
        "        # Run an Object Detection Algorithm\n",
        "        result, pred_bboxes = run_obstacle_detection(img)\n",
        "\n",
        "        if len(pred_bboxes)>0:\n",
        "            #If we have boxes, get the Optical Flow we ran before\n",
        "            fl_vec = fl_vectors[idx-1]\n",
        "            fl_out = cv2.resize(add_arrow_to_box(result, pred_bboxes, fl_vec), (1248,376))\n",
        "            fl_img = fl_images[idx-1]\n",
        "            img_final = np.concatenate([fl_out, fl_img], axis=0)\n",
        "        final_vid.append(img_final)\n",
        "        out = cv2.VideoWriter(\"output.mp4\",cv2.VideoWriter_fourcc(*'mp4v'), 15.0, (fl_out.shape[1] ,fl_out.shape[0]))\n",
        "        for i in range(len(final_vid)):\n",
        "            out.write(final_vid[i].astype(np.uint8))\n",
        "        out.release()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}