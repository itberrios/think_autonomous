{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FlowNet.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM7f2bLKHq6yvvjGvAdF7Uo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jeremy26/optical_flow_course/blob/main/FlowNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVdTJsytfFL9"
      },
      "source": [
        "# FlowNet Expert – Deep Learning for Optical Flow Workshop\n",
        "\n",
        "Welcome to FlowNet! In this project, we're going to build a FlowNet algorithm with PyTorch! The idea is simple, given two images, output the optical flow!\n",
        "<p>\n",
        "\n",
        "![flownet](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQ_p_REZwjQ1YqfV51j8vQ1qJodRUDRI8Dd7tPuwbWW-tWUQBhKibGi3Bq1ox6SNp5k2ts&usqp=CAU)\n",
        "\n",
        "In this project, we're going to:\n",
        "\n",
        "1.   **Load and Prepare the Dataset** for the Model\n",
        "2.   Define a **FlowNet Architecture**\n",
        "3.   **Train the Model** on KITTI\n",
        "4.   **Run the Model**\n",
        "\n",
        "Just a note before we begin, this code has been adapted from Clement Pinard who authored FlowNet PyTorch. I have been in contact with clement numerous times and he helped me make this course and this code easy to get. \n",
        "<p>\n",
        "\n",
        "For your information, [here is the original repo link](https://github.com/ClementPinard/FlowNetPytorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UvxRbhEhgDl"
      },
      "source": [
        "Let's begin with some synchronization and imports!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIyNEHieihui"
      },
      "source": [
        "!wget https://thinkautonomous-flownet.s3.eu-west-3.amazonaws.com/flownet-data.zip && unzip flownet-data.zip && rm flownet-data.zip\n",
        "!mkdir output\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZFfHIHefoc6"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_ynPeGbfXoO"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "from google.colab.patches import cv2_imshow\n",
        "import glob\n",
        "from __future__ import division\n",
        "import os.path\n",
        "import os\n",
        "from imageio import imread\n",
        "import numbers\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import random\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import torch.utils.data as data\n",
        "import torch.utils.data as data\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.nn.init import kaiming_normal_, constant_\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9Wav9zINSlK"
      },
      "source": [
        "# Part I - Load and Prepare the Dataset for the Model\n",
        "\n",
        "There are a few Optical Flow Datasets we can use:\n",
        "\n",
        "*   Flying Chairs\n",
        "*   Scene Flow (KITTI)\n",
        "*   Middleburry\n",
        "*   MPI Sintel\n",
        "*   Kinetics\n",
        "\n",
        "For the purpose of this course, we'll use the [KITTI Dataset](http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=flow) as it's the closest to autonomous driving."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tS0JmLEIzV6J"
      },
      "source": [
        "images_dataset = sorted(glob.glob(\"dataset/images_2/*.png\"))\n",
        "labels_dataset = sorted(glob.glob(\"dataset/flow_occ/*.png\"))\n",
        "\n",
        "print(len(images_dataset))\n",
        "print(len(labels_dataset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVTt3HHJ8VZS"
      },
      "source": [
        "### 1.1 – Understand input/labels\n",
        "\n",
        "Here's what we want:\n",
        "*   **Input:** A pair of 2 consecutive images\n",
        "*   **Labels:** The Flow Map\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xd6VvYSO2Sm9"
      },
      "source": [
        "\"\"\"\n",
        "Make a List of (Img1, Img2, Flow Map)\n",
        "\"\"\"\n",
        "\n",
        "images = []\n",
        "for flow_map in labels_dataset:\n",
        "    root_filename = flow_map[-13:-7]\n",
        "    img1 = os.path.join(\"dataset/images_2/\", root_filename+'_10.png')\n",
        "    img2 = os.path.join(\"dataset/images_2/\", root_filename+'_11.png')\n",
        "    images.append([[img1, img2], flow_map])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0X24OY0qa3bU"
      },
      "source": [
        "print(images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsYolQyRLCBV"
      },
      "source": [
        "def bgr2rgb(image):\n",
        "    \"\"\"\n",
        "    Convert BGR TO RGB\n",
        "    \"\"\"\n",
        "    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbviE9fCbFlA"
      },
      "source": [
        "cv2_imshow(bgr2rgb(cv2.imread(images[0][1])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBom2OfZuAAZ"
      },
      "source": [
        "# SIMPLE WORKAROUND\n",
        "yuv = cv2.imread(images[0][1], -1)\n",
        "rgb_map = cv2.cvtColor(yuv, cv2.COLOR_YUV2RGB) \n",
        "rgb_map[np.where((rgb_map==[0,0,0]).all(axis=2))] = [255,255,255]\n",
        "plt.imshow(rgb_map)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYcLLCoEmwLO"
      },
      "source": [
        "!pip install pypng\n",
        "from read_kitti import read_png_file, flow_to_image\n",
        "# Huge Thanks: https://github.com/liruoteng/OpticalFlowToolkit/tree/master/lib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODRSJrlkWHR4"
      },
      "source": [
        "\"\"\"\n",
        "Let's read a triplet of images!\n",
        "\"\"\"\n",
        "idx = random.randint(0,200)\n",
        "\n",
        "image_t0 = bgr2rgb(cv2.imread(images[idx][0][0]))\n",
        "image_t1 = bgr2rgb(cv2.imread(images[idx][0][1]))\n",
        "flo_path = images[idx][1]\n",
        "\n",
        "flow_label = read_png_file(flo_path)\n",
        "rgb_map = flow_to_image(flow_label)\n",
        "\n",
        "\"\"\"\n",
        "Visualize the Data\n",
        "\"\"\"\n",
        "\n",
        "f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(30,20))\n",
        "ax1.imshow(image_t0)\n",
        "ax1.set_title('Image t0', fontsize=30)\n",
        "ax2.imshow(image_t1)\n",
        "ax2.set_title('Image t1', fontsize=30)\n",
        "ax3.imshow(rgb_map)\n",
        "ax3.set_title(\"Flow (label)\", fontsize=30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiVJ7r4RCktJ"
      },
      "source": [
        "### 1.2 – Split the Data into Train/Test\n",
        "We're going to split the dataset into training and testing. A good ratio would be 80% training and 20% testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mm4Ael2nKEA5"
      },
      "source": [
        "def train_test_split(images, default_split=0.8):\n",
        "    \"\"\"\n",
        "    Splits the Dataset Paths into Train/Test\n",
        "    \"\"\"\n",
        "    split_values = np.random.uniform(0,1,len(images)) < default_split # Randomly decides if an image is train or test\n",
        "    train_samples = [sample for sample, split in zip(images, split_values) if split]\n",
        "    test_samples = [sample for sample, split in zip(images, split_values) if not split]\n",
        "    return train_samples, test_samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nu2jUoJiFUf1"
      },
      "source": [
        "train_samples, test_samples = train_test_split(images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haF5_QQKwIlV"
      },
      "source": [
        "print(len(train_samples))\n",
        "print(len(test_samples))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ml6j9bW3C158"
      },
      "source": [
        "### 1.3 – Load the Images\n",
        "\n",
        "So far, we have:\n",
        "*   *images* – a list of triplet paths\n",
        "*   *train_samples* and *test_samples* – these paths into two sets\n",
        "\n",
        "Now, we need to **convert these lists of paths into actual tensors** of images for PyTorch. We'll also need to **perform some transform operations** such as **normalization, random cropping,** etc...\n",
        "<p>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gaKhCMpwAJA"
      },
      "source": [
        "def load_flow_from_png(png_path):\n",
        "    '''\n",
        "    This is used to read flow label images from the KITTI Dataset\n",
        "    '''\n",
        "    flo_file = cv2.imread(png_path, -1) # The Image is a 16 Bit Image. We must read it with OpenCV and the flag cv2.IMREAD_UNCHANGED (-1)\n",
        "    flo_img = flo_file[:,:,2:0:-1].astype(np.float32)\n",
        "\n",
        "    # See the README File in the KITTI DEVKIT AND THE FLOW READER FUNCTIONS\n",
        "    invalid = (flo_file[:,:,0] == 0)\n",
        "    flo_img = flo_img - 32768\n",
        "    flo_img = flo_img / 64\n",
        "\n",
        "    # Valid and Small Flow = 1e-10\n",
        "    flo_img[np.abs(flo_img) < 1e-10] = 1e-10\n",
        "\n",
        "    # Invalid Flow = 0\n",
        "    flo_img[invalid, :] = 0\n",
        "    return flo_img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGhJaSV_N0Wy"
      },
      "source": [
        "def KITTI_loader(root,path_imgs, path_flo):\n",
        "    \"\"\"\n",
        "    Returns the Loaded Images in RGB, and the Loaded Optical Flow Labels\n",
        "    \"\"\"\n",
        "    imgs = [os.path.join(root,path) for path in path_imgs]\n",
        "    flo = os.path.join(root,path_flo)\n",
        "    return [cv2.imread(img)[:,:,::-1].astype(np.float32) for img in imgs], load_flow_from_png(flo)#read_png_file(flo)[:,:,2:0:-1].astype(np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5FzlcKifku4"
      },
      "source": [
        "import flow_transforms\n",
        "\n",
        "div_flow = 20 #Factor by which we divide the output (thus >=1). It makes training more stable to deal with low numbers than big ones.\n",
        "\n",
        "#Normalized for the Flying Chair Dataset (https://github.com/ClementPinard/FlowNetPytorch/issues/101#issuecomment-805222823)\n",
        "input_transform = transforms.Compose([flow_transforms.ArrayToTensor(), transforms.Normalize(mean=[0,0,0], std=[255,255,255]), transforms.Normalize(mean=[0.45,0.432,0.411], std=[1,1,1])])\n",
        "\n",
        "target_transform = transforms.Compose([flow_transforms.ArrayToTensor(),transforms.Normalize(mean=[0,0],std=[div_flow,div_flow])])\n",
        "\n",
        "co_transform = flow_transforms.Compose([flow_transforms.RandomCrop((320,448)), flow_transforms.RandomVerticalFlip(),flow_transforms.RandomHorizontalFlip()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZzU_k7JOnh_"
      },
      "source": [
        "class ListDataset(data.Dataset):\n",
        "    def __init__(self, path_list, transform=None, target_transform=None, co_transform=None, loader=KITTI_loader):\n",
        "        self.root = os.getcwd()\n",
        "        self.path_list = path_list\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.co_transform = co_transform\n",
        "        self.loader = loader\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        In Python, __getitem__ is used to read values from a class. For example; read the transformed input files.\n",
        "        Instead of calling the function .read(), we use __getitem__ to directly get the value.\n",
        "        Similarly, __setitem__ can be used to fill values in a class.\n",
        "        \"\"\"\n",
        "        inputs, target = self.path_list[index]\n",
        "        inputs, target = self.loader(self.root, inputs, target)\n",
        "        if self.co_transform is not None:\n",
        "            inputs, target = self.co_transform(inputs, target)\n",
        "        if self.transform is not None:\n",
        "            inputs[0] = self.transform(inputs[0])\n",
        "            inputs[1] = self.transform(inputs[1])\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "        return inputs, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.path_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpFjzrw4Q-fb"
      },
      "source": [
        "train_dataset = ListDataset(train_samples, input_transform, target_transform, co_transform, loader=KITTI_loader)\n",
        "\n",
        "test_dataset = ListDataset(test_samples, input_transform,target_transform, flow_transforms.CenterCrop((370,1224)), loader=KITTI_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZWy6zozKiPl"
      },
      "source": [
        "If you don't understand the ListDataset and how the __ getitem() __ works, a good exercise would be to try and re-code this entire class into something you understand better. Otherwise, let's move on with the Optical Flow Network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eujz-vKtfp8F"
      },
      "source": [
        "# Part II – Build a FlowNet Architecture\n",
        "\n",
        "FlowNet has two variations:\n",
        "*   **FlownetS** or Simple, which is a simple version using 2D Convolutions to get to the optical flow computation\n",
        "*   **FlownetC** or Correlated, which adds a correlation layer and process images separately\n",
        "\n",
        "In both, there are two main parts:\n",
        "*   An **Encoder** Part, learning features\n",
        "*   A **Refinement** Part, playing the decoder and creating the output Flow Mask.\n",
        "\n",
        "It looks like we've got some work! In this workshop, we'll build the FlowNet S architecture, as the researchers mentioned it worked best on KITTI! You can find the implementation for the FlowNet C architecture in the course for your information.\n",
        "<p>\n",
        "Here's a look at the flownet S model:\n",
        "\n",
        "![flownets](https://miro.medium.com/max/1400/0*XVygX0wF3enVQJLe.)  \n",
        "\n",
        "And the refinement part:<p>\n",
        "\n",
        "![refinement](https://i1.wp.com/syncedreview.com/wp-content/uploads/2017/09/image-14.png?fit=692%2C268&ssl=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GzQjnE1LdWU"
      },
      "source": [
        "### 2.1 – Code the necessary operations\n",
        "\n",
        "Operations we'll need:\n",
        "* Convolutions in 2D (with or without batchnorm)\n",
        "* Output Flow Prediction\n",
        "* Transposed Convolutions\n",
        "* Crop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmFL7wBaX7If"
      },
      "source": [
        "#Define a Convolution with or without batchnorm and LeakyReLU\n",
        "\n",
        "def conv(batchNorm, in_planes, out_planes, kernel_size=3, stride=1):\n",
        "    if batchNorm:\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=(kernel_size-1)//2, bias=False),\n",
        "            nn.BatchNorm2d(out_planes),\n",
        "            nn.LeakyReLU(0.1,inplace=True)\n",
        "        )\n",
        "    else:\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=(kernel_size-1)//2, bias=True),\n",
        "            nn.LeakyReLU(0.1,inplace=True)\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MdJOXrKYJh9"
      },
      "source": [
        "#Define the last convolution (optical flow map prediction)\n",
        "def predict_flow(in_planes):\n",
        "    return nn.Conv2d(in_planes,2,kernel_size=3,stride=1,padding=1,bias=False) # Note: In the paper, a kernel Size of 3 is written; but their implementation uses 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Mazjo3pYPjv"
      },
      "source": [
        "#Define a Deconvolution\n",
        "def deconv(in_planes, out_planes):\n",
        "    return nn.Sequential(\n",
        "        nn.ConvTranspose2d(in_planes, out_planes, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "        nn.LeakyReLU(0.1,inplace=True)\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQ1u-2DMfn5j"
      },
      "source": [
        "#Define a Cropping Operation\n",
        "def crop_like(input, target):\n",
        "    if input.size()[2:] == target.size()[2:]:\n",
        "        return input\n",
        "    else:\n",
        "        return input[:, :, :target.size(2), :target.size(3)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XSKEoTOMkUl"
      },
      "source": [
        "### 2.2 – Create the FlowNet S Model\n",
        "\n",
        "What PyTorch needs to create a model:\n",
        "\n",
        "*   An **__init __() function** with a list of all the operations. Weights can be initialized here.\n",
        "*   A **forward()** function that will take an input and compute the flow. A note: In case of training, we want to return all flows, in case of testing, we only want the output flow.\n",
        "*   **Weights and Biases**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VARY7bulbCV"
      },
      "source": [
        "class FlowNetS(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self,batchNorm=True):\n",
        "        super(FlowNetS,self).__init__()\n",
        "\n",
        "        #ENCODER PART\n",
        "        self.batchNorm = batchNorm\n",
        "        self.conv1   = conv(self.batchNorm,   6,   64, kernel_size=7, stride=2)\n",
        "        self.conv2   = conv(self.batchNorm,  64,  128, kernel_size=5, stride=2)\n",
        "        self.conv3   = conv(self.batchNorm, 128,  256, kernel_size=5, stride=2)\n",
        "        self.conv3_1 = conv(self.batchNorm, 256,  256)\n",
        "        self.conv4   = conv(self.batchNorm, 256,  512, stride=2)\n",
        "        self.conv4_1 = conv(self.batchNorm, 512,  512)\n",
        "        self.conv5   = conv(self.batchNorm, 512,  512, stride=2)\n",
        "        self.conv5_1 = conv(self.batchNorm, 512,  512)\n",
        "        self.conv6   = conv(self.batchNorm, 512, 1024, stride=2)\n",
        "        self.conv6_1 = conv(self.batchNorm,1024, 1024) # Note: This one doesn't exist in the paper, but it does in their implementation\n",
        "\n",
        "        #REFINEMENT PART\n",
        "        self.deconv5 = deconv(1024,512)\n",
        "        self.deconv4 = deconv(1026,256)\n",
        "        self.deconv3 = deconv(770,128)\n",
        "        self.deconv2 = deconv(386,64)\n",
        "\n",
        "        self.predict_flow6 = predict_flow(1024)\n",
        "        self.predict_flow5 = predict_flow(1026)\n",
        "        self.predict_flow4 = predict_flow(770)\n",
        "        self.predict_flow3 = predict_flow(386)\n",
        "        self.predict_flow2 = predict_flow(194)\n",
        "\n",
        "        self.upsampled_flow6_to_5 = nn.ConvTranspose2d(2, 2, 4, 2, 1, bias=False)\n",
        "        self.upsampled_flow5_to_4 = nn.ConvTranspose2d(2, 2, 4, 2, 1, bias=False)\n",
        "        self.upsampled_flow4_to_3 = nn.ConvTranspose2d(2, 2, 4, 2, 1, bias=False)\n",
        "        self.upsampled_flow3_to_2 = nn.ConvTranspose2d(2, 2, 4, 2, 1, bias=False)\n",
        "\n",
        "        \n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "                # Initialize the Convolutions with \"He Initialization\" to 0.1 (https://arxiv.org/pdf/1502.01852.pdf)\n",
        "                kaiming_normal_(m.weight, 0.1)\n",
        "                if m.bias is not None:\n",
        "                    # Initialize all bias to 0\n",
        "                    constant_(m.bias, 0)\n",
        "            # Initialize the BatchNorm Convolutions with \"He Initialization\" to 1 (https://arxiv.org/pdf/1502.01852.pdf)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                constant_(m.weight, 1)\n",
        "                constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #ENCODER\n",
        "        out_conv2 = self.conv2(self.conv1(x))\n",
        "        out_conv3 = self.conv3_1(self.conv3(out_conv2))\n",
        "        out_conv4 = self.conv4_1(self.conv4(out_conv3))\n",
        "        out_conv5 = self.conv5_1(self.conv5(out_conv4))\n",
        "        out_conv6 = self.conv6_1(self.conv6(out_conv5))\n",
        "        \n",
        "        #REFINEMENT\n",
        "        flow6       = self.predict_flow6(out_conv6)\n",
        "        flow6_up    = crop_like(self.upsampled_flow6_to_5(flow6), out_conv5)\n",
        "        out_deconv5 = crop_like(self.deconv5(out_conv6), out_conv5)\n",
        "\n",
        "        concat5 = torch.cat((out_conv5,out_deconv5,flow6_up),1)\n",
        "        flow5       = self.predict_flow5(concat5)\n",
        "        flow5_up    = crop_like(self.upsampled_flow5_to_4(flow5), out_conv4)\n",
        "        out_deconv4 = crop_like(self.deconv4(concat5), out_conv4)\n",
        "\n",
        "        concat4 = torch.cat((out_conv4,out_deconv4,flow5_up),1)\n",
        "        flow4       = self.predict_flow4(concat4)\n",
        "        flow4_up    = crop_like(self.upsampled_flow4_to_3(flow4), out_conv3)\n",
        "        out_deconv3 = crop_like(self.deconv3(concat4), out_conv3)\n",
        "\n",
        "        concat3 = torch.cat((out_conv3,out_deconv3,flow4_up),1)\n",
        "        flow3       = self.predict_flow3(concat3)\n",
        "        flow3_up    = crop_like(self.upsampled_flow3_to_2(flow3), out_conv2)\n",
        "        out_deconv2 = crop_like(self.deconv2(concat3), out_conv2)\n",
        "\n",
        "        concat2 = torch.cat((out_conv2,out_deconv2,flow3_up),1)\n",
        "        flow2 = self.predict_flow2(concat2)\n",
        "\n",
        "        if self.training:\n",
        "            return flow2,flow3,flow4,flow5,flow6\n",
        "        else:\n",
        "            return flow2\n",
        "\n",
        "    def weight_parameters(self):\n",
        "        return [param for name, param in self.named_parameters() if 'weight' in name]\n",
        "\n",
        "    def bias_parameters(self):\n",
        "        return [param for name, param in self.named_parameters() if 'bias' in name]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "746dLmS5NayY"
      },
      "source": [
        "### 2.3 – Create an Empty FlowNet S Model (with or without pretrained weights)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "celCeoeVeFYK"
      },
      "source": [
        "#Define FlowNet S\n",
        "def flownets(data=None, batchNorm=False):\n",
        "    \"\"\"FlowNetS model architecture from the\n",
        "    \"Learning Optical Flow with Convolutional Networks\" paper (https://arxiv.org/abs/1504.06852)\n",
        "    Args:\n",
        "        data : pretrained weights of the network. will create a new one if not set\n",
        "    \"\"\"\n",
        "    model = FlowNetS(batchNorm=batchNorm)\n",
        "    if data is not None:\n",
        "        model.load_state_dict(data['state_dict'])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ekw_YP0BOMEW"
      },
      "source": [
        "If you'd like to create an empty model, simply call the flownets() function without parameters.\n",
        "\n",
        "⚠️ However, **the KITTI Dataset only has 200 data points**. It might be very hard to converge.\n",
        "\n",
        "👉 A good solution is to **load the weights already trained on the Flying Chair dataset** by Clement Pinard, and then **finetune the model on KITTI**. It's called Transfer Learning; and can also work in these cases when the dataset is poor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWz3NPoFjX21"
      },
      "source": [
        "model_to_load = \"models/flownets_bn_EPE2.459.pth.tar\"\n",
        "#model_to_load = \"models/model_best.pth.tar\"\n",
        "checkpoint = torch.load(model_to_load) #if CPU use second parameter: map_location=torch.device(\"cpu\")\n",
        "model = flownets(data=checkpoint, batchNorm=True)\n",
        "\n",
        "print(model)#model = flownets()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Pn2GBm7hu86"
      },
      "source": [
        "# Part III - Train the Model on KITTI\n",
        "To train a Deep Learning Model, we'll need:\n",
        "\n",
        "*   Data\n",
        "*   A Model\n",
        "*   Parameters\n",
        "*   A Loss Function\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiS3HNkBh_g6"
      },
      "source": [
        "### 3.1 – Define Hyperparameters and Variables\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wq6GuSleh6GR"
      },
      "source": [
        "arch = \"flownetsbn\"\n",
        "solver = \"adam\" # or sgd\n",
        "epochs = 200\n",
        "epoch_size = 0\n",
        "batch_size = 64\n",
        "learning_rate = 10e-4\n",
        "workers = 4\n",
        "pretrained = None\n",
        "bias_decay = 0\n",
        "weight_decay = 4e-4\n",
        "momentum = 0.9 # Momentum for SGD - Alpha for Adam\n",
        "milestones= [100,150,200] # Epochs by which we divide learning rate by 2\n",
        "\n",
        "save_path = '{},{},{}epochs,b{},lr{}'.format(arch, solver, epochs,batch_size,learning_rate)\n",
        "\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "\n",
        "param_groups = [{'params': model.bias_parameters(), 'weight_decay': bias_decay},\n",
        "                {'params': model.weight_parameters(), 'weight_decay': weight_decay}]\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if device.type == \"cuda\":\n",
        "    model = torch.nn.DataParallel(model).cuda()\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "if solver == 'adam':\n",
        "    optimizer = torch.optim.Adam(param_groups, learning_rate,betas=(momentum, 0.999)) # In the paper, Adam is used\n",
        "elif solver == 'sgd':\n",
        "    optimizer = torch.optim.SGD(param_groups, learning_rate,momentum=momentum)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEgGYm8tUYbe"
      },
      "source": [
        "Writers can be used to plug values or for tensorboard visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhY4EmpsUVVq"
      },
      "source": [
        "train_writer = SummaryWriter(os.path.join(save_path,'train'))\n",
        "test_writer = SummaryWriter(os.path.join(save_path,'test'))\n",
        "output_writers = []\n",
        "\n",
        "for i in range(3):\n",
        "    output_writers.append(SummaryWriter(os.path.join(save_path,'test',str(i))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-Fdk24_iItq"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=workers, pin_memory=True, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,num_workers=workers, pin_memory=True, shuffle=False)\n",
        "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtrOORO5km0P"
      },
      "source": [
        "### 3.2 – Define the Loss Function as the End Point Error (EPE)\n",
        "\n",
        "Flownet (and most optical flow algorithms) use the end point error (EPE) as a metric for the loss function.\n",
        "It is simply the euclidean distance between the real value (ground truth) and the predicted one.<p>\n",
        "EPE = ![](https://latex.codecogs.com/gif.latex?%5Cinline%20%5Cleft%20%5C%7CV_%7Best%7D%20-%20V_%7Bgt%7D%20%5Cright%20%5C%7C)\n",
        "\n",
        "As the model outputs different flow maps, at different scales, we'll need to create different EPE functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EtAjWaBtNVt"
      },
      "source": [
        "def EPE(input_flow, target_flow, sparse=False, mean=True):\n",
        "    EPE_map = torch.norm(target_flow-input_flow,2,1)\n",
        "    batch_size = EPE_map.size(0)\n",
        "    if sparse:\n",
        "        # invalid flow is defined with both flow coordinates to be exactly 0\n",
        "        mask = (target_flow[:,0] == 0) & (target_flow[:,1] == 0)\n",
        "        EPE_map = EPE_map[~mask]\n",
        "    if mean:\n",
        "        return EPE_map.mean()\n",
        "    else:\n",
        "        return EPE_map.sum()/batch_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TGjqGFQpEix"
      },
      "source": [
        "def realEPE(output, target, sparse=False):\n",
        "    b, _, h, w = target.size()\n",
        "    upsampled_output = F.interpolate(output, (h,w), mode='bilinear', align_corners=False) # used to resize the output\n",
        "    return EPE(upsampled_output, target, sparse, mean=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBy4PpK2s8pn"
      },
      "source": [
        "def sparse_max_pool(input, size):\n",
        "    '''Downsample the input by considering 0 values as invalid.\n",
        "    Unfortunately, no generic interpolation mode can resize a sparse map correctly,\n",
        "    the strategy here is to use max pooling for positive values and \"min pooling\"\n",
        "    for negative values, the two results are then summed.\n",
        "    This technique allows sparsity to be minized, contrary to nearest interpolation,\n",
        "    which could potentially lose information for isolated data points.'''\n",
        "\n",
        "    positive = (input > 0).float()\n",
        "    negative = (input < 0).float()\n",
        "    output = F.adaptive_max_pool2d(input * positive, size) - F.adaptive_max_pool2d(-input * negative, size)\n",
        "    return output\n",
        "\n",
        "\n",
        "def multiscaleEPE(network_output, target_flow, weights=None, sparse=False):\n",
        "    def one_scale(output, target, sparse):\n",
        "\n",
        "        b, _, h, w = output.size()\n",
        "        if sparse:\n",
        "            target_scaled = sparse_max_pool(target, (h, w))\n",
        "        else:\n",
        "            target_scaled = F.interpolate(target, (h, w), mode='area')\n",
        "        return EPE(output, target_scaled, sparse, mean=False)\n",
        "    \n",
        "\n",
        "    if type(network_output) not in [tuple, list]:\n",
        "        network_output = [network_output]\n",
        "    if weights is None:\n",
        "        weights = [0.005, 0.01, 0.02, 0.08, 0.32]  # as in original article\n",
        "    assert(len(weights) == len(network_output))\n",
        "\n",
        "    loss = 0\n",
        "    for output, weight in zip(network_output, weights):\n",
        "        loss += weight * one_scale(output, target_flow, sparse)\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoKeZlN5Lga6"
      },
      "source": [
        "### 3.3 Create functions to train and validate\n",
        "\n",
        "We'll begin by using something quite common with PyTorch called an **AverageMeter()**. It is simply a class that **stores the values** for our losses, and that can do an average, median, or whatever we want. **It's quite useful in our case where we have to average a loss over several pixels and several frames.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPj4N7TCuSJ1"
      },
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{:.3f} ({:.3f})'.format(self.val, self.avg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o08yr4e5vfV1"
      },
      "source": [
        "def save_checkpoint(state, is_best, save_path, filename='checkpoint.pth.tar'):\n",
        "    \"\"\"\n",
        "    Save a checkpoint to continue training after a wifi problem 🙃\n",
        "    \"\"\"\n",
        "    torch.save(state, os.path.join(save_path,filename))\n",
        "    if is_best:\n",
        "        shutil.copyfile(os.path.join(save_path,filename), os.path.join(save_path,'model_best.pth.tar'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-OAEj_dO2VO"
      },
      "source": [
        "\"\"\"\n",
        "The Train() function is actually a function to train on ONE EPOCH.\n",
        "\"\"\"\n",
        "\n",
        "def train(train_loader, model, optimizer, epoch, train_writer):\n",
        "    global n_iter, div_flow\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    flow2_EPEs = AverageMeter()\n",
        "\n",
        "    multiscale_weights = [0.005,0.01,0.02,0.08,0.32] # from output_flow to flow6\n",
        "\n",
        "    epoch_size = len(train_loader)\n",
        "\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "\n",
        "    end = time.time()\n",
        "\n",
        "    for i, (input, target) in enumerate(train_loader):\n",
        "        # Go through the entire data loader\n",
        "        data_time.update(time.time() - end)\n",
        "\n",
        "        target = target.to(device)\n",
        "        input = torch.cat(input,1).to(device)\n",
        "\n",
        "        # Forward Pass\n",
        "        output = model(input)\n",
        "\n",
        "        # Since Target pooling is not very precise when sparse,\n",
        "        # take the highest resolution prediction and upsample it instead of downsampling target\n",
        "        h, w = target.size()[-2:]\n",
        "        output = [F.interpolate(output[0], (h,w)), *output[1:]]\n",
        "\n",
        "        # Compute Multiscale EPE (for all predict flows)\n",
        "        loss = multiscaleEPE(output, target, weights=multiscale_weights, sparse=True)\n",
        "\n",
        "        # Compute the Output EPE\n",
        "        flow2_EPE = div_flow * realEPE(output[0], target, sparse=True)\n",
        "\n",
        "        # Record loss and EPE\n",
        "        losses.update(loss.item(), target.size(0))\n",
        "        train_writer.add_scalar('train_loss', loss.item(), n_iter)\n",
        "        flow2_EPEs.update(flow2_EPE.item(), target.size(0))\n",
        "\n",
        "        # compute gradient and do optimization step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        if i % 1 == 0:\n",
        "            # Every 2 steps, print the Loss and EPE\n",
        "            print('Epoch: [{0}][{1}/{2}]\\t Time {3}\\t Data {4}\\t Loss {5}\\t EPE {6}'\n",
        "                  .format(epoch, i, epoch_size, batch_time,\n",
        "                          data_time, losses, flow2_EPEs))\n",
        "        n_iter += 1\n",
        "        if i >= epoch_size:\n",
        "            break\n",
        "    #Return the Average Loss and Average EPE on the Training Set\n",
        "    return losses.avg, flow2_EPEs.avg\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M29R5t1ATknN"
      },
      "source": [
        "def validate(val_loader, model, epoch, output_writers):\n",
        "    global div_flow\n",
        "    batch_time = AverageMeter()\n",
        "    flow2_EPEs = AverageMeter()\n",
        "\n",
        "    # switch to evaluate mode\n",
        "    model.eval()\n",
        "\n",
        "    end = time.time()\n",
        "    for i, (input, target) in enumerate(val_loader):\n",
        "        #Go through the entire validation loader\n",
        "\n",
        "        target = target.to(device)\n",
        "        input = torch.cat(input,1).to(device)\n",
        "\n",
        "        # Forward Pass\n",
        "        output = model(input)\n",
        "\n",
        "        #Compute the EPE\n",
        "        flow2_EPE = div_flow*realEPE(output, target, sparse=True)\n",
        "        # record EPE\n",
        "        flow2_EPEs.update(flow2_EPE.item(), target.size(0))\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        if i < len(output_writers):  # log first output of first batches\n",
        "            if epoch == 0:\n",
        "                mean_values = torch.tensor([0.45,0.432,0.411], dtype=input.dtype).view(3,1,1)\n",
        "                output_writers[i].add_image('GroundTruth', flow2rgb(div_flow * target[0], max_value=10), 0)\n",
        "                output_writers[i].add_image('Inputs', (input[0,:3].cpu() + mean_values).clamp(0,1), 0)\n",
        "                output_writers[i].add_image('Inputs', (input[0,3:].cpu() + mean_values).clamp(0,1), 1)\n",
        "            output_writers[i].add_image('FlowNet Outputs', flow2rgb(div_flow * output[0], max_value=10), epoch)\n",
        "\n",
        "        if i % 5 == 0:\n",
        "            print('Test: [{0}/{1}]\\t Time {2}\\t EPE {3}'\n",
        "                  .format(i, len(val_loader), batch_time, flow2_EPEs))\n",
        "\n",
        "    print(' * EPE {:.3f}'.format(flow2_EPEs.avg))\n",
        "    # Return Average EPE on Validation Set\n",
        "    return flow2_EPEs.avg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEng6J_8ewBP"
      },
      "source": [
        "### 3.4 – Train the Model and Visualize the Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9AfTWhbOcMm"
      },
      "source": [
        "def flow2rgb(flow_map, max_value):\n",
        "    \"\"\"\n",
        "    Used to visualize the output after a forward pass\n",
        "    https://github.com/ClementPinard/FlowNetPytorch/issues/86\n",
        "    \"\"\"\n",
        "    flow_map_np = flow_map.detach().cpu().numpy()\n",
        "    _, h, w = flow_map_np.shape\n",
        "    flow_map_np[:,(flow_map_np[0] == 0) & (flow_map_np[1] == 0)] = float('nan')\n",
        "    rgb_map = np.ones((3,h,w)).astype(np.float32)\n",
        "    if max_value is not None:\n",
        "        normalized_flow_map = flow_map_np / max_value\n",
        "    else:\n",
        "        normalized_flow_map = flow_map_np / (np.abs(flow_map_np).max())\n",
        "    rgb_map[0] += normalized_flow_map[0]\n",
        "    rgb_map[1] -= 0.5*(normalized_flow_map[0] + normalized_flow_map[1])\n",
        "    rgb_map[2] += normalized_flow_map[1]\n",
        "    return rgb_map.clip(0,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmLtwOI14Ui0"
      },
      "source": [
        "save_path = '{},{},{}epochs{},b{},lr{}'.format(arch, solver, epochs, ',epochSize'+str(epoch_size) if epoch_size > 0 else '', batch_size, learning_rate)\n",
        "n_iter = 0\n",
        "best_EPE = -1\n",
        "\n",
        "# We'll start from a model pretrained on \"Flying Chairs\" and finetune it to KITTI\n",
        "\n",
        "save_path = os.path.join(\"models\",save_path)\n",
        "\n",
        "print('=> will save everything to {}'.format(save_path))\n",
        "\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "\n",
        "for epoch in range(0, epochs):\n",
        "    scheduler.step()\n",
        "\n",
        "    # Train for one epoch\n",
        "    train_loss, train_EPE = train(train_loader, model, optimizer, epoch, train_writer)\n",
        "    train_writer.add_scalar('mean EPE', train_EPE, epoch)\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    with torch.no_grad():\n",
        "        endpointerror = validate(val_loader, model, epoch, output_writers)\n",
        "    test_writer.add_scalar('mean EPE', endpointerror, epoch)\n",
        "\n",
        "    # Store the best EPE\n",
        "    if best_EPE < 0:\n",
        "        best_EPE = endpointerror\n",
        "\n",
        "    is_best = endpointerror < best_EPE\n",
        "    best_EPE = min(endpointerror, best_EPE)\n",
        "    save_checkpoint({\n",
        "        'epoch': epoch + 1,\n",
        "        'arch': arch,\n",
        "        'state_dict': model.module.state_dict(),\n",
        "        'best_EPE': best_EPE,\n",
        "        'div_flow': div_flow\n",
        "    }, is_best, save_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nz_Z37hHwG1u"
      },
      "source": [
        "# Part IV – Run the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ddG1M4dk_UI"
      },
      "source": [
        "### 4.1 – On 2 Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dvj-o8LXZtQ3"
      },
      "source": [
        "input_transform=transforms.Compose([flow_transforms.ArrayToTensor(),\n",
        "        transforms.Normalize(mean=[0,0,0], std=[255,255,255]),\n",
        "        transforms.Normalize(mean=[0.411,0.432,0.45], std=[1,1,1])\n",
        "    ])\n",
        "\n",
        "network_data = torch.load(\"models/model_best.pth.tar\")\n",
        "#network_data = torch.load(\"models/flownetsbn,adam,200epochs,b64,lr0.001/checkpoint.pth.tar\")\n",
        "div_flow = network_data['div_flow']\n",
        "\n",
        "model = flownets(network_data, batchNorm=True).to(device)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "cudnn.benchmark = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wt24naTkpXp_"
      },
      "source": [
        "idx = random.randint(0,len(train_samples))\n",
        "img1_file = train_samples[idx][0][0]\n",
        "img2_file = train_samples[idx][0][1]\n",
        "flow_target = flow_to_image(read_png_file(train_samples[idx][1]))\n",
        "\n",
        "with torch.no_grad():\n",
        "    img1 = input_transform(imread(img1_file))\n",
        "    img2 = input_transform(imread(img2_file))\n",
        "    input_var = torch.cat([img1, img2]).unsqueeze(0)\n",
        "    input_var = input_var.to(device)\n",
        "    output = model(input_var)\n",
        "\n",
        "    for suffix, flow_output in zip(['flow', 'inv_flow'], output):\n",
        "        filename = img1_file[:-4]+\"flow\"\n",
        "        rgb_flow = flow2rgb(div_flow * flow_output, max_value=None)\n",
        "        rgb_flow= (rgb_flow * 255).astype(np.uint8).transpose(1,2,0)\n",
        "\n",
        "f, (ax0, ax1, ax2) = plt.subplots(1, 3, figsize=(30,20))\n",
        "ax0.imshow(cv2.imread(img1_file)[:,:,::-1])\n",
        "ax0.set_title(\"Original Image\", fontsize=30)\n",
        "ax1.imshow(rgb_flow)\n",
        "ax1.set_title('Prediction', fontsize=30)\n",
        "ax2.imshow(flow_target)\n",
        "ax2.set_title('Ground Truth', fontsize=30)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGOAL7KSj69H"
      },
      "source": [
        "![](https://miro.medium.com/max/592/0*tRzHPmhbfDOfH6qw.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5U7ktNXlBKg"
      },
      "source": [
        "### 4.2 – On a Video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdwDpGLMeL6n"
      },
      "source": [
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzHWqMalbqNC"
      },
      "source": [
        "video_idx = 2\n",
        "video_images = sorted(glob.glob(\"videos/video\"+str(video_idx)+\"/*.png\"))\n",
        "result_video = []\n",
        "\n",
        "for idx_run, img in enumerate(video_images):\n",
        "    if idx_run==0:\n",
        "        im1 = imread(img)\n",
        "        idx_run+=1\n",
        "    else:\n",
        "        im2 = imread(img)\n",
        "        with torch.no_grad():\n",
        "            img1 = input_transform(im1)\n",
        "            img2 = input_transform(im2)\n",
        "            input_var = torch.cat([img1, img2]).unsqueeze(0)\n",
        "            input_var = input_var.to(device)\n",
        "\n",
        "            output = model(input_var)\n",
        "\n",
        "            for suffix, flow_output in zip(['flow', 'inv_flow'], output):\n",
        "                rgb_flow = flow2rgb(div_flow * flow_output, max_value=None)\n",
        "                rgb_flow = (rgb_flow * 255).astype(np.uint8).transpose(1,2,0)\n",
        "                result_video.append(cv2.cvtColor(rgb_flow, cv2.COLOR_RGB2BGR))\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpMH93IUqHZb"
      },
      "source": [
        "out = cv2.VideoWriter(\"output/out-\"+str(video_idx)+\".mp4\",cv2.VideoWriter_fourcc(*'MP4V'), 15.0, (311 ,94))\n",
        "\n",
        "for i in range(len(result_video)):\n",
        "    out.write(result_video[i])\n",
        "out.release()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3Q_WAKIeUmH"
      },
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "mp4 = open(\"output/out-\"+str(video_idx)+\".mp4\",'rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=800 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7g09dEgptzfE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-R-VB2or1WA"
      },
      "source": [
        "video_idx = 3\n",
        "video_images = sorted(glob.glob(\"video\"+str(video_idx)+\"/*.png\"))\n",
        "vid = []\n",
        "for idx_run, img in enumerate(video_images):\n",
        "    vid.append(cv2.imread(img).astype(np.uint8))\n",
        "\n",
        "out = cv2.VideoWriter(\"output/out-\"+str(video_idx)+\".mp4\",cv2.VideoWriter_fourcc(*'MP4V'), 15.0, (1242 ,375))\n",
        "\n",
        "for i in range(len(vid)):\n",
        "    out.write(vid[i])\n",
        "out.release()\n",
        "\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "mp4 = open(\"output/out-\"+str(video_idx)+\".mp4\",'rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=800 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}