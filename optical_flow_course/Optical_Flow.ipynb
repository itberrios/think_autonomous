{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Optical Flow.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "e8U2xTKCUmMa",
        "45CB2FC4VJJ4"
      ],
      "authorship_tag": "ABX9TyNg3gTolD1zYhKLRXx/yXyu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jeremy26/video_analysis_course/blob/main/Optical_Flow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WojsJBzVqiu"
      },
      "source": [
        "# Feature Tracking & Optical Flow â€“Â Intro Workshop\n",
        "Welcome to your first workshop on Feature Tracking and Optical Flow!<p>\n",
        "\n",
        "In this workshop, we're going to explore 3 techniques to solve the tracking problem:\n",
        "*   Feature Tracking\n",
        "*   Sparse Optical Flow\n",
        "*   Dense Optical Flow\n",
        "\n",
        "In the second and third parts, we'll dive into Optical Flow. Both flow estimation techniques can help us track obstacles through time by finding the movement.\n",
        "![](https://nanonets.com/blog/content/images/2019/04/sparse-vs-dense.gif)\n",
        "\n",
        "More importantly, we'll try to understand **\"What the hell is optical flow?\"**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTCbid71KVWR"
      },
      "source": [
        "## Imports and Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ruVvZk9TPrw"
      },
      "source": [
        "!wget -qq https://optical-flow-data.s3.eu-west-3.amazonaws.com/images.zip\n",
        "!unzip -qq images.zip && rm images.zip\n",
        "!mkdir output\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvTY8TKMVAgw"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOauZ6LqIPQ4"
      },
      "source": [
        "# 1) Feature Tracking\n",
        "In this first part, we'll start by tracking visual features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vWsub15IOCL"
      },
      "source": [
        "#TODO: Load Different Image Pairs\n",
        "img1 = cv2.imread(\"images/image0.jpg\")\n",
        "img2 = cv2.imread(\"images/image1.jpg\")\n",
        "\n",
        "# INITIALIZE FAST DETECTOR AND BRIEF DESCRIPTOR\n",
        "fast = cv2.xfeatures2d.StarDetector_create()\n",
        "brief = cv2.xfeatures2d.BriefDescriptorExtractor_create()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STFp0CGqIxhK"
      },
      "source": [
        "# DETECTOR\n",
        "kp = fast.detect(img1,None)\n",
        "\n",
        "# DESCRIPTOR\n",
        "kp1, des1 = brief.compute(img1, kp)\n",
        "\n",
        "# KEYPOINTS DRAWN\n",
        "img1_kp = cv2.drawKeypoints(img1, kp1, None, color=(0,255,0), flags=0)\n",
        "cv2_imshow(img1_kp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dr1FKpszOt-j"
      },
      "source": [
        "# DETECTOR\n",
        "kp = fast.detect(img2,None)\n",
        "# DESCRIPTOR\n",
        "kp2, des2 = brief.compute(img2, kp)\n",
        "\n",
        "# KEYPOINTS DRAWN\n",
        "img2_kp = cv2.drawKeypoints(img2, kp2, None, color=(0,255,0), flags=0)\n",
        "cv2_imshow(img2_kp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZhiwTT6IWuq"
      },
      "source": [
        "#FLANN MATCHING\n",
        "FLANN_INDEX_KDTREE = 0\n",
        "index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
        "search_params = dict(checks = 50)\n",
        "\n",
        "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
        "\n",
        "matches = flann.knnMatch(np.float32(des1),np.float32(des2),k=2) # Use NP.FLOAT32 for ORB, BRIEF, etc\n",
        "\n",
        "# store all the good matches as per Lowe's ratio test.\n",
        "good = []\n",
        "for m,n in matches:\n",
        "    if m.distance < 0.7*n.distance:\n",
        "        good.append(m)\n",
        "\n",
        "if len(good)>10:\n",
        "    p1 = np.float32([ kp1[m.queryIdx].pt for m in good ]).reshape(-1,1,2)\n",
        "    p2 = np.float32([ kp2[m.trainIdx].pt for m in good ]).reshape(-1,1,2)\n",
        "\n",
        "draw_params = dict(matchColor = (0,255,0), # draw matches in green color\n",
        "                    singlePointColor = None,\n",
        "                    flags = 2)\n",
        "\n",
        "img_briefmatch = cv2.drawMatches(img1,kp1,img2,kp2,good,None,**draw_params)\n",
        "cv2_imshow(img_briefmatch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8U2xTKCUmMa"
      },
      "source": [
        "# 2) Sparse Optical Flow\n",
        "\n",
        "In this first part of the tutorial, we'll explore Sparse Optical Flow. This technique is about calculating Optical Flow only for specific features, and not for every pixel. It's **faster** but **not very accurate**.<p> \n",
        "The algorithm we'll use for Feature Detection is the **Shi-Tomasi** detector, and the one we'll use for Optical Flow is called **Lucas-Kanade**.<p>\n",
        "By the way, both algorithms are older than me; so we're really exploring the traditional techniques here ðŸ™ƒ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgW4iViuGGGa"
      },
      "source": [
        "### First, let's calculate the Sparse Optical Flow on two consecutive images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYo2DSFpmXTe"
      },
      "source": [
        "#Load two consecutive images\n",
        "im1 = cv2.imread(\"images/image0.jpg\")\n",
        "im2 = cv2.imread(\"images/image1.jpg\")\n",
        "\n",
        "cv2_imshow(im1)\n",
        "cv2_imshow(im2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyijLOEZmgw4"
      },
      "source": [
        "#Create an Empty mask and define the color of output to green\n",
        "mask = np.zeros_like(im1)\n",
        "color = (0, 255, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10UmwvkFmn6w"
      },
      "source": [
        "#Convert both images to grayscale\n",
        "gray1 = cv2.cvtColor(im1, cv2.COLOR_BGR2GRAY)\n",
        "gray2 = cv2.cvtColor(im2, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "cv2_imshow(gray1)\n",
        "cv2_imshow(gray2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5WW67aNmuBh"
      },
      "source": [
        "Define the parameters and launch the **Shi-Tomasi corner detector** on the first image. According to OpenCV: \"As usual, image should be a **grayscale image**. Then you specify **number of corners you want to find**. Then you specify the **quality level**, which is a value between 0-1, which denotes the **minimum quality of corner below which everyone is rejected**. Then we provide the **minimum euclidean distance between corners detected**.\" <p> ðŸ‘‰To understand how to define the parameters, you can use [this link](https://docs.opencv.org/master/dd/d1a/group__imgproc__feature.html#ga1d6bb77486c8f92d79c8793ad995d541)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNF_lziiXTsS"
      },
      "source": [
        "feature_params = dict(maxCorners = 300, qualityLevel = 0.0025, minDistance = 15, blockSize = 2) # Default 300, 0.25, 3, 7\n",
        "prev = cv2.goodFeaturesToTrack(gray1, mask = None, **feature_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46ymm-gFrELf"
      },
      "source": [
        "print(feature_params)\n",
        "print(prev[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAZj75Sopv9m"
      },
      "source": [
        "#Go through each detected corner, and draw a dot\n",
        "image_corners = im1.copy()\n",
        "\n",
        "for i in prev:\n",
        "    x,y = i.ravel()\n",
        "    cv2.circle(image_corners,(x,y),3,255,-1)\n",
        "cv2_imshow(image_corners)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tC6kweV9nSIq"
      },
      "source": [
        "Launch the Lucas Kanade Optical Flow algorithm with the features and both grayscale images.<p>\n",
        "ðŸ‘‰ [Here's the link](https://docs.opencv.org/3.4/dc/d6b/group__video__track.html#ga473e4b886d0bcc6b65831eb88ed93323) to understand the parameters better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5G8nvAYm-JK"
      },
      "source": [
        "lk_params = dict(winSize = (15,15), maxLevel = 2, criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
        "next, status, error = cv2.calcOpticalFlowPyrLK(gray1, gray2, prev, None, **lk_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2V7045frAG-"
      },
      "source": [
        "print(lk_params)\n",
        "idx = 19\n",
        "print(prev[idx])\n",
        "print(next[idx])\n",
        "print(status[idx])\n",
        "print(error[idx])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJlwmVwbnRhy"
      },
      "source": [
        "#Store the Matches (status=1 means a match)\n",
        "good_old = prev[status == 1]\n",
        "good_new = next[status == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHmvlruTngVW"
      },
      "source": [
        "#Go through each matched feature, and draw it on the second image\n",
        "\n",
        "for new, old in zip(good_new, good_old):\n",
        "    a, b = new.ravel()\n",
        "    c, d = old.ravel()\n",
        "    mask = cv2.arrowedLine(mask, (a, b), (c, d), color, 2)\n",
        "    im2 = cv2.circle(im2, (a, b), 3, color, -1)\n",
        "# Overlays the optical flow tracks on the original frame\n",
        "output = cv2.add(im2, mask)\n",
        "# Updates previous frame\n",
        "gray1 = gray2.copy()\n",
        "# Updates previous good feature points\n",
        "prev = good_new.reshape(-1, 1, 2)\n",
        "\n",
        "#cv2.imwrite(\"output.jpg\", output)\n",
        "cv2_imshow(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9oTzNU6jXg2"
      },
      "source": [
        "cv2_imshow(mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcyq-nA2GLVc"
      },
      "source": [
        "### On a video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpaoTlhhzl3O"
      },
      "source": [
        "def lk_from_image(image):\n",
        "    global idx, gray1, mask, prev\n",
        "    if idx==0:\n",
        "        gray1 = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        feature_params = dict(maxCorners = 600, qualityLevel = 0.0025, minDistance = 15, blockSize = 2)\n",
        "        prev = cv2.goodFeaturesToTrack(gray1, mask = None, **feature_params)\n",
        "        mask = np.zeros_like(image)\n",
        "        idx+=1\n",
        "        return image\n",
        "    # Else\n",
        "    im2 = image\n",
        "    if idx%15==0:\n",
        "        mask = np.zeros_like(image)\n",
        "    \n",
        "    if idx%5==0:\n",
        "        # Every 5 images, relaunch the feature detection\n",
        "        feature_params = dict(maxCorners = 300, qualityLevel = 0.2, minDistance = 2, blockSize = 7)\n",
        "        prev = cv2.goodFeaturesToTrack(gray1, mask = None, **feature_params)\n",
        "    \n",
        "    color = (0, 255, 0)\n",
        "    gray2 = cv2.cvtColor(im2, cv2.COLOR_BGR2GRAY)\n",
        " \n",
        "    lk_params = dict(winSize = (15,15), maxLevel = 2, criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
        "    next, status, error = cv2.calcOpticalFlowPyrLK(gray1, gray2, prev, None, **lk_params)\n",
        "\n",
        "    good_old = prev[status == 1]\n",
        "    good_new = next[status == 1]\n",
        "\n",
        "    for new, old in zip(good_new, good_old):\n",
        "        a, b = new.ravel()\n",
        "        c, d = old.ravel()            \n",
        "        mask = cv2.arrowedLine(mask, (a, b), (c, d), color, 2)\n",
        "        im2 = cv2.circle(im2, (a, b), 3, color, -1)\n",
        "    output = cv2.add(im2, mask)\n",
        "    gray1 = gray2.copy()\n",
        "    prev = good_new.reshape(-1, 1, 2)\n",
        "    idx += 1\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_HM_XOgzJYl"
      },
      "source": [
        "from moviepy.editor import VideoFileClip\n",
        "idx = 0\n",
        "\n",
        "video_file = \"images/skateboard.mp4\"\n",
        "clip = VideoFileClip(video_file).subclip(0,20)\n",
        "white_clip = clip.fl_image(lk_from_image)\n",
        "%time white_clip.write_videofile(\"output/output_lk_skateboard.mp4\",audio=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMEJCuVqYXv2"
      },
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "mp4 = open('output/output_lk_skateboard.mp4','rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=800 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45CB2FC4VJJ4"
      },
      "source": [
        "# 3) Dense Optical Flow\n",
        "\n",
        "In this second part, we'll try to learn about Dense Optical Flow; which is used when **we're tracking every pixel of an image**. It's **longer** but **works much better**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7ZMGK0Wbym9"
      },
      "source": [
        "### Let's do the Dense Optical Flow on two consecutive images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIHnxWVWUnRg"
      },
      "source": [
        "#Load the 2 consecutive images and convert them to grayscale\n",
        "im1 = cv2.imread(\"images/image0.jpg\")\n",
        "im2 = cv2.imread(\"images/image1.jpg\")\n",
        "gray1 = cv2.cvtColor(im1, cv2.COLOR_BGR2GRAY)\n",
        "gray2 = cv2.cvtColor(im2, cv2.COLOR_BGR2GRAY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PevCO2NHvIm"
      },
      "source": [
        "#Create a mask in HSV Color Space.\n",
        "hsv = np.zeros_like(im1)\n",
        "# Sets image saturation to maximum.\n",
        "hsv[..., 1] = 255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0fxiNT4K8sW"
      },
      "source": [
        "Now comes the Optical Flow Algorithm. This time, we'll use the Farneback algorithm (2003).<p>\n",
        "Unlike the Lucas Kanade method, we don't want to find \"next, status, and error\". This time, we just want to estimate one output, the **flow**.\n",
        "\n",
        "If you'd like to understand how to tweak the parameters, you can [visit this link](https://docs.opencv.org/3.0-beta/modules/video/doc/motion_analysis_and_object_tracking.html#calcopticalflowfarneback). <p>\n",
        "\n",
        "ðŸ‘‰ The next step is to calculate this flow, and convert it into a visual output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ox1jVRYmH6lz"
      },
      "source": [
        "flow = cv2.calcOpticalFlowFarneback(gray1, gray2, None, 0.5, 3, 15, 3, 5, 1.2, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiIB36h_OSjj"
      },
      "source": [
        "### But what is the Flow?\n",
        "**The Flow is a 2D Matrix** that has the same size as our input frame (image 1). In this matrix, we have the **optical flow vectors U and V**. These are made of point coordinates. **For any point P (x,y) in the grayscale image, the flow contains the corresponding (delta_x, delta_y)** --- how much did that pixel move in the X and Y direction.<p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQMpRIqMOmIe"
      },
      "source": [
        "print(\"Shapes Gray & Flow\")\n",
        "print(gray1.shape)\n",
        "print(flow.shape)\n",
        "print(\" \")\n",
        "\n",
        "print(\"Flow Output\")\n",
        "print(flow)\n",
        "print(\" \")\n",
        "\n",
        "print(\"U Vector\")\n",
        "print(flow[...,0])\n",
        "print(\" \")\n",
        "\n",
        "print(\"V Vector\")\n",
        "print(flow[...,1])\n",
        "print(\" \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLf5oiW0S6fx"
      },
      "source": [
        "**We have 2 vectors U and V**, which correspond to the first and second column of the flow output in cartesian coordinates.<p>\n",
        "\n",
        "---\n",
        "\n",
        "ðŸ‘‰ According to OpenCV: \"We get **a 2-channel array** with **optical flow vectors, (u,v)**. We find their **magnitude** and **direction**. We color code the result for better visualization. **Direction corresponds to Hue** value of the image. **Magnitude corresponds to Value plane**.\"<p>\n",
        "\n",
        "---\n",
        "\n",
        "ðŸ‘‰ In other words, it's possible to make something with \"the point moved 2 pixels to the right\"! We find the Magnitude and Angle, convert that to HSV Space (Hue, Saturation, Value) and then make it an image.\n",
        "\n",
        "I know this sounds horrible, but we need to go back to High School Maths (or maybe college), and remember about Vectors in Cartesian and Polar coordinates. Of course, **the vectors represent the displacements of each pixels.**\n",
        "\n",
        "*   A vector in **cartesian** coordinates has its values in **(x,y)**.\n",
        "*   A vector in **polar** coordinates has its values in **Radian**.\n",
        "![](https://cdn.kastatic.org/ka-perseus-images/1559d8785a298fdd0bac0443388b3812c4327ec3.png)\n",
        "\n",
        "<p> To compute the Polar coordinates, we must calculate the magnitude and the angle of the vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCkGfYSIWwwr"
      },
      "source": [
        "# Computes the magnitude and angle of the 2D vectors\n",
        "magnitude, angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
        "\n",
        "print(magnitude)\n",
        "print(angle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5Uxlt-UIBFW"
      },
      "source": [
        "# Sets image hue according to the optical flow direction\n",
        "hsv[..., 0] = angle * 180 / np.pi / 2\n",
        "\n",
        "print(hsv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DkdQmhrXdhI"
      },
      "source": [
        "# Sets image value according to the optical flow magnitude (normalized)\n",
        "hsv[..., 2] = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX)\n",
        "print(hsv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__66OysqXlcW"
      },
      "source": [
        "cv2_imshow(hsv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fo-ZyD1tXi_d"
      },
      "source": [
        "# Converts HSV to RGB (BGR) color representation\n",
        "rgb = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvxszBO4IQec"
      },
      "source": [
        "#Display the Output\n",
        "cv2.imwrite(\"output/Dense Output.jpg\",rgb)\n",
        "cv2_imshow(rgb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsEYF8adPaqw"
      },
      "source": [
        "![](https://www.researchgate.net/profile/Christophoros-Nikou/publication/266149545/figure/fig1/AS:392088710598656@1470492641144/The-optical-flow-field-color-coding-Smaller-vectors-are-lighter-and-color-represents-the.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoNDuRbpb1OK"
      },
      "source": [
        "### On a video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbZTvKwnVXQ-"
      },
      "source": [
        "def farneback_from_image(image):\n",
        "    global idx, im1\n",
        "    if idx==0:\n",
        "        im1 = image\n",
        "        idx+=1\n",
        "        return im1\n",
        "    else:\n",
        "        im2 = image\n",
        "        gray1 = cv2.cvtColor(im1, cv2.COLOR_BGR2GRAY)\n",
        "        gray2 = cv2.cvtColor(im2, cv2.COLOR_BGR2GRAY)\n",
        "        hsv = np.zeros_like(im1)\n",
        "        hsv[..., 1] = 255\n",
        "        flow = cv2.calcOpticalFlowFarneback(gray1, gray2, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
        "        magnitude, angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
        "        hsv[..., 0] = angle * 180 / np.pi / 2\n",
        "        hsv[..., 2] = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX)\n",
        "        bgr = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n",
        "        idx+=1\n",
        "        im1= image.copy()\n",
        "        return bgr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wN0eWvOl2qtj"
      },
      "source": [
        "from moviepy.editor import VideoFileClip\n",
        "idx = 0\n",
        "\n",
        "video_file = \"images/skateboard.mp4\"\n",
        "clip = VideoFileClip(video_file).subclip(0,10)\n",
        "white_clip = clip.fl_image(farneback_from_image)\n",
        "%time white_clip.write_videofile(\"output/output_farneback.mp4\",audio=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fSC4AhGPn0S"
      },
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "mp4 = open('output/output_farneback.mp4','rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=800 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iCc7IQ5Yy7w"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}